Strategická analýza a metodika generování výzkumných otázek pro řešení epických problémů: Neuro-symbolická syntéza v projektovém řízení
1. Úvod: Anatomie epického problému a informační entropie
V současném prostředí výzkumu a vývoje, ať už se jedná o akademickou sféru, farmaceutický průmysl nebo pokročilé softwarové inženýrství, narážíme na limity tradičního lineárního plánování. Projekty, které označujeme termínem "epické" (v terminologii agilního řízení "Epics"), se vyznačují fundamentálně odlišnou strukturou než standardní inženýrské úkoly. Epický problém není definován pouze svým rozsahem, ale především svou inherentní neurčitostí a nelineární strukturou závislostí. Zatímco u stavby mostu je kauzalita a posloupnost kroků (fundamenty – pilíře – mostovka) deterministická a známá předem, u epického problému – například vývoje nového materiálu pro supravodiče nebo návrhu obecné umělé inteligence – je struktura řešení emergentní. Odhaluje se teprve v průběhu řešení samotného.
Tato zpráva předkládá hloubkovou analýzu metodik pro automatizované generování a prioritizaci dalších kroků (otázek) v řešení takových problémů. Cílem je identifikovat mechanismy, které maximalizují pravděpodobnost konvergence k řešení v co nejkratším čase a s minimálními náklady. Analýza se pohybuje na průsečíku teorie grafů, teorie informace, strojového učení a strategického projektového řízení.
1.1 Definice stavového prostoru a role otázky
Epický problém lze matematicky formalizovat jako prohledávání stavového prostoru o vysoké dimenzionalitě. Počáteční stav představuje současné znalosti a zdroje, cílový stav představuje vyřešení problému. Prostor mezi nimi je vyplněn "mlhou války" – neznámými proměnnými, stochastickými procesy a skrytými závislostmi. V tomto kontextu není "otázka" pouhým jazykovým konstruktem, ale operátorem, který transformuje stavový prostor. Položení a zodpovězení relevantní otázky funguje jako "řez" (pruning) rozhodovacím stromem – eliminuje celé větve neperspektivních hypotéz, nebo naopak odhaluje nové, dříve neviditelné uzly (podproblémy).
Základní metrikou pro hodnocení kvality otázky v tomto systému je očekávaný informační zisk (Expected Information Gain - EIG). V teorii informace je entropie mírou neurčitosti náhodné proměnné. Cílem řešení epického problému je redukce globální entropie systému na nulu (plné poznání řešení). Efektivní generátor otázek tedy musí fungovat jako optimalizační algoritmus, který v každém kroku vybírá takovou akci (otázku), jejíž marginální přínos k redukci entropie je maximální vzhledem k nákladům na její zodpovězení.
1.2 Dynamika kritické cesty v neurčitém prostředí
V klasickém projektovém řízení (metoda CPM - Critical Path Method) je kritická cesta definována jako nejdelší sekvence závislých úkolů určující minimální dobu trvání projektu. U epických problémů je však kritická cesta dynamická a stochastická. Zodpovězení jedné fundamentální otázky (např. "Je sloučenina X stabilní při teplotě Y?") může okamžitě zneplatnit tisíce následných úkolů (pokud je odpověď "ne"), nebo naopak vygenerovat nové úzké hrdlo projektu. Statické Ganttovy diagramy zde selhávají.
Analýza se proto zaměřuje na přístupy, které modelují problém jako Orientovaný acyklický graf (Directed Acyclic Graph - DAG) pravděpodobnostních závislostí. V tomto modelu není cílem pouze "dokončit úkoly", ale "odstranit nejistotu z kritické cesty". Efektivita výběru otázek je pak měřena schopností systému identifikovat uzly v grafu, které mají nejvyšší "centralitu rizika" – tedy ty body, jejichž nevyřešení blokuje největší objem potenciálního progresu.
1.3 Ekonomický imperativ a Trade-off analýza
Klíčovým aspektem zadání je cena realizace a provozu. Musíme rozlišovat mezi "levným generováním" (např. pomocí LLM, kde cena za token klesá) a "drahým ověřováním" (např. laboratorní experiment nebo vývoj prototypu). Systém, který vygeneruje tisíc plausibilních, ale slepých uliček, je ekonomicky devastující, přestože samotné vygenerování stálo pár centů. Naopak systém s vysokými počátečními náklady na implementaci (např. konstrukce robustního znalostního grafu), který však eliminuje 30 % zbytečných experimentů, nabízí masivní návratnost investic (ROI). V následujících kapitolách budeme důsledně analyzovat tento poměr cena/výkon pro jednotlivé architektury.
________________
2. Teoretické a matematické rámce pro výběr otázek: Deterministické a stochastické modely
Předtím, než přistoupíme k analýze neuronových sítí, je nezbytné ukotvit problematiku v rigorózních matematických metodách. Tyto přístupy nabízejí transparentnost, vysvětlitelnost a prokazatelné záruky optimality, které čistě statistickým modelům (jako LLM) často chybí.
2.1 Teorie grafů: Topologická analýza problému
Reprezentace znalostí a úkolů ve formě grafu je nejrobustnějším způsobem, jak uchopit komplexitu epického problému. Uzly grafu mohou reprezentovat hypotézy, datové body, úkoly nebo logické propozice. Hrany reprezentují kauzální nebo logické závislosti (např. "Hypotéza A je prerekvizitou pro Hypotézu B").
2.1.1 Topologické třídění (Topological Sorting) jako nástroj sekvencování
Pro orientované acyklické grafy (DAG), které jsou přirozeným modelem pro kauzální závislosti ve výzkumu, existuje lineární uspořádání uzlů takové, že pro každou hranu od $u$ do $v$ platí, že $u$ předchází $v$ v uspořádání. Algoritmy jako Kahnův algoritmus nebo metody založené na prohledávání do hloubky (DFS) umožňují toto uspořádání nalézt v lineárním čase $O(V+E)$.
V kontextu generování otázek to znamená, že pokud máme sadu potenciálních otázek a známe jejich závislosti, topologické třídění nám poskytne matematicky korektní "učební plán" nebo "roadmapu". Systém nemůže navrhnout otázku týkající se optimalizace procesu, pokud nebyla zodpovězena otázka týkající se proveditelnosti procesu, protože v grafu existuje orientovaná hrana. Rozšířením o prioritní fronty (priority queues) lze do topologického třídění vnést i váhy důležitosti, což umožňuje generovat sekvence, které nejen respektují logiku, ale i preference řešitele.
2.1.2 Metriky centrality: Identifikace strategických uzlů
Ne všechny otázky mají stejnou váhu. Teorie grafů nabízí sofistikované metriky centrality, které identifikují uzly s největším strukturálním dopadem.
* Betweenness Centrality (Centralita mezilehlosti): Tato metrika kvantifikuje, kolikrát daný uzel leží na nejkratší cestě mezi dvěma jinými uzly. Ve výzkumném grafu otázka s vysokou betweenness centralitou funguje jako "most" mezi různými sub-doménami problému. Její vyřešení má potenciál propojit disparátní části znalostní báze a umožnit transfer informací. Pro epické problémy, které jsou často interdisciplinární, je tato metrika klíčová pro identifikaci integračních bodů.
* Information Centrality (Informační centralita): Na rozdíl od prosté vzdálenosti bere v úvahu všechny možné cesty v síti a jejich "odpor" (resistance distance). Interpretuje graf jako elektrický obvod. Uzel s vysokou informační centralitou má nejlepší pozici pro šíření nebo sběr informací z celé sítě. V kontextu řízení projektu to znamená, že selhání nebo úspěch v tomto uzlu se nejrychleji propaguje do zbytku systému. Identifikace takových otázek je prioritou pro řízení rizik.
2.2 Informační teorie a Active Learning: Měření nejistoty
Zatímco grafy modelují strukturu, teorie informace modeluje obsahovou nejistotu. Shannonova entropie je fundamentálním měřítkem množství informace (nebo neurčitosti) obsažené v náhodné proměnné.
2.2.1 Entropy Sampling a Uncertainty Sampling
V metodikách Active Learning (Aktivní učení) se využívá strategie výběru instancí, pro které je model nejméně jistý. Pro znalostní grafy, kde se snažíme predikovat chybějící vztahy (Link Prediction), to znamená vybírat hrany, u kterých je pravděpodobnost existence blízká 0,5 (maximální entropie).
Matematicky pro binární klasifikaci vztahu je entropie $H(x)$ definována jako:




$$H(x) = - (p \log p + (1-p) \log (1-p))$$


kde $p$ je predikovaná pravděpodobnost existence vztahu.
Výzkum ukazuje, že strategie založené na entropii (Entropy Sampling) jsou vysoce efektivní pro doplňování znalostních grafů (Knowledge Graph Completion), protože nutí systém prozkoumávat "hranice" svých znalostí. Nicméně, v čisté formě může tato metoda vést k problému "hlučné televize" – systém zkoumá jevy, které jsou inherentně náhodné a nepredikovatelné, a tedy jejich zkoumání nepřináší užitek pro finální cíl.
2.2.2 Bayesovský experimentální design a Value of Information (VoI)
Abychom předešli plýtvání zdroji na irelevantní nejistoty, musíme zavést koncept Hodnoty informace (Value of Information - VoI). VoI je definována jako rozdíl mezi očekávaným užitkem rozhodnutí učiněného s novou informací a užitkem rozhodnutí učiněného bez ní.




$$VoI(e) = \mathbb{E}_y [\max_a \mathbb{E}_\theta [U(a, \theta) | y, e]] - \max_a \mathbb{E}_\theta [U(a, \theta)]$$


Kde $e$ je experiment (otázka), $y$ je výsledek, $a$ je akce a $U$ je užitková funkce.
V kontextu epického projektu je užitkovou funkcí typicky "dokončení projektu v čase $T$". VoI analýza tedy prioritizuje otázky, jejichž zodpovězení má největší potenciál zkrátit kritickou cestu nebo snížit rozptyl odhadu dokončení. Aplikace Bayesovských sítí umožňuje aktualizovat pravděpodobnostní rozdělení parametrů modelu na základě nových důkazů a dynamicky přepočítávat VoI pro zbývající otázky.
2.3 Probabilistické řízení projektů: PERT a Monte Carlo
Tradiční deterministická metoda kritické cesty (CPM) selhává v prostředí výzkumu, kde délka trvání úkolů není fixní. Metoda PERT (Program Evaluation and Review Technique) zavádí probabilistický přístup pomocí Beta distribuce definované třemi body: optimistický čas ($O$), nejpravděpodobnější čas ($M$) a pesimistický čas ($P$).
Střední hodnota trvání $\mu$ a směrodatná odchylka $\sigma$ se odhadují jako:




$$\mu = \frac{O + 4M + P}{6}, \quad \sigma = \frac{P - O}{6}$$
Integrace PERT s metodami Monte Carlo simulace umožňuje generovat tisíce scénářů průběhu projektu. Systém pak může identifikovat tzv. "kritické indexy" (criticality indices) pro jednotlivé úkoly – pravděpodobnost, že daný úkol bude ležet na kritické cestě. Otázky týkající se úkolů s vysokým kritickým indexem a vysokou variancí ($\sigma$) by měly být generovány a řešeny prioritně, protože představují největší riziko pro termín dokončení. Data ukazují, že tento přístup, ač výpočetně náročnější než CPM, poskytuje mnohem realističtější odhady v nejistém prostředí a umožňuje proaktivní alokaci zdrojů.
________________
3. Neuronové sítě a subsymbolické přístupy: Generativní síla a sémantická flexibilita
Zatímco algoritmické metody excelují v optimalizaci struktury, často postrádají schopnost "porozumět" obsahu a generovat nové, kreativní hypotézy mimo předem definované schéma. Zde nastupují neuronové sítě, zejména Velké jazykové modely (LLM) a Grafové neuronové sítě (GNN).
3.1 Large Language Models (LLM) jako generátory hypotéz
LLM (jako GPT-4, Claude 3.5, DeepSeek) představují revoluci v sémantickém generování otázek. Díky tréninku na obrovských korpusech textů dokáží provádět analogické uvažování a propojovat koncepty z vzdálených domén, což je pro řešení epických problémů klíčové.
3.1.1 Chain-of-Thought (CoT) a dekompozice problémů
LLM excelují v technikách jako Chain-of-Thought (CoT), kde model generuje postupné kroky uvažování vedoucí k závěru. Pro generování otázek to znamená, že model dokáže simulovat myšlenkový proces experta: "Pokud chceme dosáhnout X, musíme nejprve ověřit Y. K ověření Y potřebujeme znát Z. Tedy, první otázka je: Jaké jsou vlastnosti Z?". Tato schopnost rekurzivní dekompozice je zásadní pro rozbití epického problému na řešitelné podúlohy. Výzkum ukazuje, že CoT výrazně zvyšuje schopnost modelů řešit komplexní úlohy vyžadující plánování.
3.1.2 Problém halucinací a nespolehlivosti
Hlavním limitem LLM je absence ukotvení v pravdě (grounding). Modely fungují na principu pravděpodobnostní predikce tokenů, nikoliv na logickém odvozování nad fakty. To vede k "halucinacím" – generování otázek, které znějí vysoce odborně a plausibilně, ale jsou založeny na neexistujících předpokladech (např. dotaz na vlastnosti neexistující chemické sloučeniny). V kontextu řízení projektu to představuje kritické riziko: pokud tým stráví zdroje na zkoumání halucinované slepé uličky, efektivita projektu klesá. Proto nemohou být LLM použity jako autonomní plánovače bez externí verifikace.
3.1.3 Ekonomika provozu: Fenomén "LLMflation"
Z hlediska nákladů dochází k fenoménu zvanému "LLMflation" – exponenciálnímu poklesu ceny za jednotku inteligence (token). Ceny inferencí klesají řádově desítky procent ročně, zatímco výkon modelů roste. Například model GPT-4o-mini dosahuje výkonu srovnatelného se staršími vlajkovými modely za zlomek ceny. To otevírá možnost využívat LLM hrubou silou (brute-force generation) – vygenerovat tisíce variant otázek a následně je filtrovat levnějším, specializovaným modelem nebo algoritmem. Náklady na generování otázek se tak stávají marginálními ve srovnání s náklady na jejich experimentální ověření.
3.2 Reinforcement Learning (RL): Strategické prohledávání grafů
Posilované učení (Reinforcement Learning - RL) nabízí rámec pro trénování agentů, kteří se učí sekvenčnímu rozhodování za účelem maximalizace odměny.
3.2.1 Multi-hop Reasoning a DeepPath
V kontextu znalostních grafů se RL agenti (např. modely jako DeepPath nebo MultiKR) učí navigovat mezi entitami. Agent začíná v uzlu reprezentujícím počáteční problém a jeho "akcí" je výběr sousedního uzlu (vztahu), který chce prozkoumat. Odměna (reward) je definována dosažením cílové entity nebo snížením nejistoty. Na rozdíl od náhodných procházek (Random Walk) nebo BFS se RL agent učí heuristiky a vzorce specifické pro danou doménu, což mu umožňuje nacházet relevantní cesty (sekvence otázek) mnohem efektivněji. Tyto modely prokázaly schopnost najít kauzální řetězce v biomedicínských grafech, které byly pro lidské experty neintuitivní.
3.2.2 Hierarchické RL (HRL) pro dlouhodobé plánování
Epické problémy vyžadují dlouhý horizont plánování, kde standardní RL trpí problémem řídkých odměn (sparse rewards) – agent dostane odměnu až po vyřešení celého problému, což může trvat tisíce kroků. Hierarchické RL řeší tento problém zavedením více úrovní abstrakce. "Manažer" (Meta-controller) generuje abstraktní pod-cíle (např. "analyzuj tržní data"), zatímco "Dělník" (Controller) generuje konkrétní sekvenci akcí k dosažení pod-cíle. Tento přístup je nezbytný pro udržení koherence strategie v dlouhém časovém úseku a umožňuje agentovi efektivněji prozkoumávat stavový prostor.
3.3 Graph Neural Networks (GNN): Strukturální intuice
GNN (např. GraphSAGE, GAT) se učí nízko-dimenzionální reprezentace (embeddingy) uzlů na základě jejich topologického okolí.
3.3.1 Link Prediction jako generátor otázek
V neúplném znalostním grafu může GNN predikovat pravděpodobnost existence hrany mezi dvěma uzly, i když tato hrana nebyla v trénovacích datech. Vysoká pravděpodobnost u neexistující hrany je silným signálem pro generování výzkumné otázky: "Existuje tento vztah ve skutečnosti?". GNN dokáží zachytit složité nelineární vzorce v grafu (např. tranzitivní vztahy, homofilie), které unikají jednoduchým heuristikám. Knihovny jako PyTorch Geometric (PyG) nebo Deep Graph Library (DGL) poskytují optimalizované implementace těchto modelů škálovatelné na miliony uzlů.
3.3.2 Srovnání GNN vs. LLM
Zatímco GNN jsou výpočetně efektivní a pracují s přesnou strukturou, postrádají sémantickou hloubku. Nevědí, co uzel znamená, jen kde je. LLM naopak rozumí významu, ale ztrácejí se ve struktuře. Analýza nákladů a přínosů ukazuje, že pro čistě strukturální úlohy (např. predikce interakcí proteinů) jsou GNN řádově levnější a přesnější než LLM. Pro generování srozumitelných otázek v přirozeném jazyce jsou však LLM nenahraditelné. Optimem se jeví hybridní přístupy, kde GNN filtruje kandidáty a LLM je verbalizuje.
________________
4. Neuro-symbolické a hybridní architektury: Cesta k optimu
Syntéza předchozích kapitol vede k jednoznačnému závěru: pro řešení epických problémů není dostačující ani čistě symbolický (rigidní), ani čistě neuronový (náchylný k chybám) přístup. Budoucnost patří Neuro-symbolické AI (Neuro-Symbolic AI), která kombinuje učíci se schopnosti a intuici neuronových sítí s logickou strukturou a verifikovatelností symbolických systémů.
4.1 Architektura Planner-Executor-Critic
Moderní agentní systémy (Agentic AI) pro vědecké objevy, jako je ChemCrow nebo GPT-Researcher, implementují architekturu rozdělených rolí, která simuluje kognitivní procesy lidského týmu.
1. Planner (Plánovač): Tato komponenta, často založená na LLM (např. GPT-4), je zodpovědná za strategické rozhodování. Využívá techniky jako Tree of Thoughts nebo ReAct (Reasoning + Acting) k dekompozici hlavního cíle na podúlohy. Planner udržuje kontext celého projektu a rozhoduje, jaký nástroj použít v dalším kroku. Klíčové je, že Planner neprovádí výpočty ani nehledá fakta přímo – pouze deleguje úkoly.
2. Executor (Vykonavatel/Nástroje): Sada specializovaných modulů. Může to být vyhledávač (Web Search), Python interpret pro výpočty, rozhraní do databáze (SQL/GraphDB) nebo simulátor. ChemCrow například využívá nástroje pro syntézu molekul, kontrolu bezpečnosti a vyhledávání v patentech. Executor vrací deterministické výsledky.
3. Critic (Kritik/Evaluátor): Často opomíjená, ale kritická součást. Kritik analyzuje výstupy Executora, ověřuje jejich konzistenci s cílem a aktualizuje stav znalostí (např. zápisem do znalostního grafu). Kritik může zamítnout navržený krok, pokud porušuje logická omezení nebo bezpečnostní pravidla.
Tato architektura umožňuje LLM "přemýšlet" v mantinelech definovaných symbolickým systémem. Pokud Planner navrhne chemickou reakci, která porušuje valenci prvků, symbolický validátor (součást Executora) ji zamítne a donutí Planner přehodnotit strategii. Tím se eliminuje většina halucinací a zajišťuje se, že generované otázky jsou fyzikálně a logicky validní.
4.2 GraphRAG: Kontextualizace generování
GraphRAG (Graph Retrieval-Augmented Generation) představuje pokročilou formu RAG, která překonává limity vektorového vyhledávání. V klasickém RAG se dokumenty dělí na textové úseky (chunky), které se vyhledávají podle sémantické podobnosti. To funguje pro lokální dotazy ("Co říká dokument A?"), ale selhává u globálních dotazů ("Jaká jsou hlavní témata napříč všemi dokumenty?").
GraphRAG nejprve extrahuje z textů entity a vztahy a buduje znalostní graf. Poté využívá algoritmů detekce komunit (např. Leiden algoritmus) k vytvoření hierarchických souhrnů grafu. Při generování otázek systém "vidí" celou strukturu komunity konceptů. To umožňuje generovat otázky, které cílí na chybějící vazby v logice nebo na rozpory mezi různými zdroji informací.
* Efektivita: GraphRAG umožňuje tzv. "holistické porozumění" datasetu. Pro epický problém to znamená schopnost identifikovat strategické mezery ve znalostech, které by při pouhém čtení izolovaných dokumentů zůstaly skryty. Ačkoliv je konstrukce grafu nákladná (mnoho volání LLM), následná inference je vysoce efektivní a poskytuje mnohem relevantnější kontext pro rozhodování.
4.3 Autonomní vědecké smyčky: AI Scientist
Systém "AI Scientist" (představený společností Sakana AI) posouvá koncept generování otázek na úroveň plné autonomie. Systém implementuje uzavřenou smyčku:
1. Idea Generation: LLM generuje nové výzkumné nápady (otázky) na základě existující literatury. Využívá evoluční algoritmy k mutaci a křížení nápadů s cílem zvýšit jejich novost a zajímavost.
2. Experimental Design & Execution: Agent napíše kód (např. v Pythonu) pro ověření hypotézy a spustí ho.
3. Visualization & Reporting: Výsledky jsou zpracovány a sepsány do formátu vědeckého článku.
4. Automated Peer Review: Jiná instance LLM funguje jako oponent, který hodnotí kvalitu článku podle standardních kritérií (novost, validita, jasnost).
Tento systém demonstruje sílu Active Learning v praxi. Automatizovaný oponent funguje jako "fitness funkce", která selektuje pouze ty nápady, které jsou slibné. Zavedení této kritické zpětné vazby dramaticky zvyšuje kvalitu generovaných otázek a simuluje proces vědeckého diskurzu.
________________
5. Analýza nákladů, provozu a efektivity (Cost-Benefit Analysis)
Rozhodnutí o volbě architektury musí být podloženo ekonomickou rozvahou. Následující tabulky a analýzy porovnávají jednotlivé přístupy.
5.1 Finanční a výpočetní náklady (OpEx & CapEx)
Metoda
	Počáteční náklady (Setup/CapEx)
	Provozní náklady (OpEx/Run)
	Škálovatelnost (Objem dat)
	Algoritmické (Grafy/CPM)
	Vysoké. Vyžaduje expertní definici ontologií, čištění dat a strukturování. Tvorba "ground truth" grafu je časově náročná.
	Nízké. Grafové algoritmy (topological sort, centrality) jsou výpočetně efektivní (polynomiální čas). Běží na běžném CPU.
	Vysoká. Grafové DB (Neo4j) škálují na miliardy hran.
	Čisté LLM (API)
	Nízké. Zero-shot prompting nevyžaduje trénink. Integrace přes API je rychlá.
	Střední až Vysoké. Platba za každý token. U komplexních agentů (smyčky myšlenek) náklady rostou lineárně s počtem kroků a délkou kontextu.
	Vysoká, ale omezena context window a latencí API.
	Vlastní LLM (Local)
	Střední. Investice do HW (GPU) nebo cloud GPU instancí. Engineering pro fine-tuning a RAG pipeline.
	Nízké (marginální). Pouze cena elektřiny. Žádné platby za tokeny. Ideální pro vysoký objem dotazů.
	Omezena dostupným VRAM a počtem GPU.
	Neuro-symbolické (Hybrid)
	Vysoké. Kombinuje infrastrukturu pro grafy, vektorové DB i LLM orchestraci. Nejsložitější na vývoj a údržbu.
	Optimalizované. LLM se volá jen pro náročné sémantické úlohy, levné grafové algoritmy řeší strukturu.
	Nejlepší poměr cena/výkon pro komplexní úlohy díky eliminaci zbytečných LLM volání.
	Dopad LLMflation: Trend exponenciálního poklesu cen LLM API (např. GPT-4o-mini je o řády levnější než původní GPT-4 při vyšší rychlosti) posouvá bod zvratu. Dříve ekonomicky neúnosné metody (např. nechat LLM číst a extrahovat data z každého dokumentu zvlášť pro stavbu grafu) se stávají proveditelnými. To favorizuje hybridní metody, kde LLM slouží jako "levný procesor textu" pro krmení robustních grafových algoritmů.
5.2 Efektivita z pohledu řízení projektů (Project Management Efficiency)
Z pohledu projektového manažera není efektivita měřena počtem vygenerovaných otázek, ale jejich dopadem na dokončení projektu (Completion).
* Exploitation (Vytěžování) vs. Exploration (Objevování):
   * Algoritmické metody jsou mistry exploitation. Pokud je struktura problému alespoň částečně známa, algoritmy jako CPM a VoI neomylně identifikují nejkratší cestu k cíli. Poskytují jistotu a kontrolu. Selhávají však v situacích, kdy řešení leží "mimo graf" (out-of-distribution).
   * LLM a RL jsou nástroje pro exploration. Dokáží navrhnout spojení, která v datech explicitně neexistují. Jsou nezbytné v raných fázích epických projektů, kdy je "mapa území" prázdná.
* Řízení rizik: Hybridní systémy nabízejí nejlepší profil rizika. Symbolická část systému (graf závislostí) slouží jako "bezpečnostní síť", která brání LLM v halucinování nesmyslných plánů. Tím se minimalizuje riziko plýtvání zdroji na slepé uličky, což je v PM terminologii eliminace "waste" (Lean methodology).
Závěr pro PM: Pro epické problémy je nutné začít s LLM pro exploraci stavového prostoru a "rozmlžení války", následně zafixovat nalezené poznatky do znalostního grafu a aplikovat algoritmické metody (Topological Sort, Centrality) pro rigorózní prioritizaci exekuce. Tento cyklus (Explore -> Structure -> Prioritize -> Execute) se musí neustále opakovat.
________________
6. Implementační strategie a technologický stack
Pro praktickou realizaci doporučujeme otevřený a modulární technologický stack, který umožňuje integrovat výhody všech zmíněných přístupů.
6.1 Technologie a nástroje
1. Znalostní báze a Grafy:
   * NetworkX (Python): Ideální pro in-memory analýzu grafů, výpočet centrality a topologické třídění. Je standardem v akademické sféře a snadno integrovatelný s ostatními Python knihovnami. Pro vizualizaci menších grafů dostačuje.
   * Neo4j: Pro perzistenci a práci s velkými grafy (miliony uzlů). Nabízí nativní algoritmy pro graph data science (GDS).
   * PyTorch Geometric (PyG): Pokud je potřeba zapojit GNN pro predikci vztahů. Nabízí hotové implementace modelů jako GraphSAGE nebo GAT.
2. Orchestrace a Workflow:
   * n8n: Low-code nástroj pro automatizaci workflow. Umožňuje vizuálně propojovat API volání (např. OpenAI), spouštění Python skriptů (pro výpočty entropie) a databázové operace. Je to "lepidlo", které drží systém pohromadě. Díky podpoře LangChain v n8n lze snadno vytvářet komplexní agenty.
   * LangChain / LangGraph: Python frameworky pro stavbu kognitivních architektur. Umožňují definovat logiku agentů, paměť a nástroje.
3. Uživatelské rozhraní a Personal Knowledge Management (PKM):
   * Obsidian: Slouží jako frontend pro výzkumníka. Poznámky v Markdownu tvoří přirozený základ znalostního grafu.
   * Pluginy: Dataview pro dotazování nad metadaty, Smart Connections pro sémantické vyhledávání, a především Local REST API, které umožňuje externím skriptům (z n8n nebo Pythonu) číst a zapisovat do vaultu. Tím se z Obsidianu stává živý dashboard výzkumného projektu.
6.2 Konkrétní implementační workflow (Příklad)
Níže je popsán doporučený proces (pipeline) pro generování a prioritizaci otázek:
1. Ingest & Extraction (Sběr a Extrakce):
   * Vstup: Nové poznámky v Obsidianu, PDF články, experimentální data.
   * Proces: n8n triggeruje workflow. LLM (přes API) analyzuje text a extrahuje entity a kauzální vztahy ("A způsobuje B", "C je nutné pro D").
   * Výstup: Aktualizace znalostního grafu v NetworkX/Neo4j.
2. Hypothesis Generation (Generování hypotéz):
   * Proces: Algoritmus v Pythonu identifikuje v grafu "díry" (chybějící hrany, izolované komunity).
   * LLM dostane prompt s kontextem těchto mezer a vygeneruje sadu kandidátních otázek (např. "Jaký je vztah mezi entitou X v clusteru A a entitou Y v clusteru B?").
3. Prioritization & VoI Calculation (Filtrace):
   * Proces: Pro každou kandidátní otázku se spočítá:
      * Entropie: Jak moc je model nejistý ohledně odpovědi? (pomocí GNN nebo uncertainty promptingu).
      * Centralita: Jaký je Betweenness a Information Centrality dotčených uzlů v grafu?
      * Kritická cesta: Leží daný uzel na aktuální kritické cestě projektu (CPM)?
   * Výstup: Skóre Priority = w1*Entropy + w2*Centrality + w3*Criticality.
4. Reporting & Feedback (Uzavření smyčky):
   * Proces: Top 5 otázků s nejvyšším skóre je zapsáno zpět do Obsidianu (např. do souboru Research_Priorities.md) formou úkolů.
   * Výzkumník si vybere úkol, provede ho a zapíše výsledek. Tím se cyklus uzavírá, graf se aktualizuje a entropie klesá.
________________
7. Závěr a doporučení
Řešení epických problémů vyžaduje posun od intuice k systémovému inženýrství znalostí. Analýza prokázala, že spoléhat se pouze na lidskou paměť nebo jednoduché "to-do" listy je v prostoru s vysokou neurčitostí neefektivní. Stejně tak je riskantní spoléhat se pouze na generativní AI bez symbolické kontroly.
Klíčová doporučení:
1. Implementujte hybridní systém: Využijte sílu LLM pro sémantickou práci (čtení, psaní, navrhování) a sílu algoritmů (grafy, statistika) pro řízení struktury a priorit.
2. Vizualizujte závislosti: Převeďte svůj problém na orientovaný acyklický graf (DAG). To je jediný způsob, jak objektivně vidět kritickou cestu v komplexním projektu.
3. Automatizujte "Peer Review": Zaveďte do svého workflow agenta (Kritika), který automaticky oponuje vašim nápadům a plánům. Je to levná a efektivní metoda kontroly kvality.
4. Optimalizujte na informační zisk: Při výběru dalšího kroku se neptejte "Co je nejjednodušší?", ale "Která otázka nejvíce zredukuje mou nejistotu ohledně finálního výsledku?".
Tento přístup transformuje roli výzkumníka z "manažera chaosu" na "architekta systému", který naviguje stavovým prostorem problému s matematickou přesností.