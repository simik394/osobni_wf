[
  {
    "name": "sessions/17238673325211131443",
    "title": "URGENT: Manual Conflict Resolution for PR #77. Fetch branch `tools-142-dashboard`. Merge origin/main. Resolve conflicts. Force Push. See https://napoveda.youtrack.cloud/issue/TOOLS-142",
    "createTime": "2026-01-20T18:29:27.041738134Z",
    "updateTime": "2026-01-20T18:35:31.804929Z",
    "state": "IN_PROGRESS",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "tools-142-dashboard"
      },
      "environmentVariablesEnabled": true
    },
    "prompt": "URGENT: Manual Conflict Resolution for PR #77. Fetch branch `tools-142-dashboard`. Merge origin/main. Resolve conflicts. Force Push. See https://napoveda.youtrack.cloud/issue/TOOLS-142",
    "url": "https://jules.google.com/session/17238673325211131443",
    "id": "17238673325211131443"
  },
  {
    "name": "sessions/7706177815247182528",
    "title": "URGENT: Manually resolve conflicts in PR #87 (status-sessions). Fetch origin, merge origin/main. RESOLVE CONFLICTS in `cmd/jules-cli/main.go` and `client.go`. RUN `go build ./cmd/jules-cli` to verify. COMMIT and PUSH FORCE. See https://napoveda.youtrack.cloud/issue/TOOLS-139",
    "createTime": "2026-01-20T18:29:23.002871512Z",
    "updateTime": "2026-01-20T18:35:27.386871Z",
    "state": "IN_PROGRESS",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "tools-139-session-state-dashboard-14217855041591550383"
      },
      "environmentVariablesEnabled": true
    },
    "prompt": "URGENT: Manually resolve conflicts in PR #87 (status-sessions). Fetch origin, merge origin/main. RESOLVE CONFLICTS in `cmd/jules-cli/main.go` and `client.go`. RUN `go build ./cmd/jules-cli` to verify. COMMIT and PUSH FORCE. See https://napoveda.youtrack.cloud/issue/TOOLS-139",
    "url": "https://jules.google.com/session/7706177815247182528",
    "id": "7706177815247182528"
  },
  {
    "name": "sessions/2593094350632980325",
    "title": "URGENT: Manual Conflict Resolution for PR #84.\nFetch branch `jules-go-publish-async-10336333082179132854`. Merge origin/main. Resolve conflicts. Force Push.",
    "createTime": "2026-01-20T18:06:19.003846587Z",
    "updateTime": "2026-01-20T18:35:35.271886Z",
    "state": "IN_PROGRESS",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "jules-go-publish-async-10336333082179132854"
      },
      "environmentVariablesEnabled": true
    },
    "prompt": "URGENT: Manual Conflict Resolution for PR #84.\nFetch branch `jules-go-publish-async-10336333082179132854`. Merge origin/main. Resolve conflicts. Force Push.",
    "url": "https://jules.google.com/session/2593094350632980325",
    "id": "2593094350632980325"
  },
  {
    "name": "sessions/16352230664676810526",
    "title": "URGENT: Manual Conflict Resolution for PR #83.\nFetch branch `tools-43-refactor-rsrch-cli-7452984589346160295`. Merge origin/main. Resolve conflicts. Force Push.",
    "createTime": "2026-01-20T18:06:18.159530264Z",
    "updateTime": "2026-01-20T18:35:45.578833Z",
    "state": "IN_PROGRESS",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "tools-43-refactor-rsrch-cli-7452984589346160295"
      },
      "environmentVariablesEnabled": true
    },
    "prompt": "URGENT: Manual Conflict Resolution for PR #83.\nFetch branch `tools-43-refactor-rsrch-cli-7452984589346160295`. Merge origin/main. Resolve conflicts. Force Push.",
    "url": "https://jules.google.com/session/16352230664676810526",
    "id": "16352230664676810526"
  },
  {
    "name": "sessions/17366877462193147751",
    "title": "URGENT: Manual Conflict Resolution for PR #77.\nFetch branch `tools-142-dashboard`. Merge origin/main. Resolve conflicts. Force Push.",
    "createTime": "2026-01-20T18:06:17.516744102Z",
    "updateTime": "2026-01-20T18:32:44.401599Z",
    "state": "IN_PROGRESS",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "tools-142-dashboard"
      },
      "environmentVariablesEnabled": true
    },
    "prompt": "URGENT: Manual Conflict Resolution for PR #77.\nFetch branch `tools-142-dashboard`. Merge origin/main. Resolve conflicts. Force Push.",
    "url": "https://jules.google.com/session/17366877462193147751",
    "id": "17366877462193147751"
  },
  {
    "name": "sessions/4324009622645823838",
    "title": "PM Orchestration Phase 1",
    "createTime": "2026-01-20T02:10:36.724213016Z",
    "updateTime": "2026-01-20T04:17:46.571827Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "main"
      },
      "environmentVariablesEnabled": true
    },
    "prompt": "You are the Lead Engineer for the Jules Project.\nYour objective is to implement Phase 1 of the PM Orchestration Tooling and ensuring the integrity of previous tasks.\n\n1. Cleanup & Audit (TOOLS-139, 140, 141, 142)\n- TOOLS-142 (Dashboard): PR #77 exists. Review it.\n- TOOLS-139, 140, 141: These sessions are marked completed but PRs are missing.\n  - Find their Session IDs (titles: \"Session state dashboard\", \"Async bulk publish\", \"YouTrack sync\").\n  - Run `jules publish <ID>` to create PRs.\n  - Ensure they are merged into `main`.\n\n2. Implement New Features (JULES-10, 11, 12, 7)\nRefer to `docs/technical_proposals_2026_01.md`.\n\nA. jules diff (JULES-10)\n- Goal: Fast session review without local checkout.\n- Tasks:\n  1. Create Windmill Script `f/jules/get_session_diff`.\n     - Inputs: `session_id`, `repo_url`.\n     - Logic: Clone repo, `jules remote pull`, `git diff`.\n  2. Update `jules-go` CLI:\n     - Add `diff` command.\n     - Call Windmill script.\n     - Render output with syntax highlighting (`github.com/alecthomas/chroma`).\n\nB. jules delegate (JULES-11)\n- Goal: Automate session creation from YouTrack.\n- Tasks:\n  1. Create Windmill Script `f/jules/delegate_task_from_youtrack`.\n     - Inputs: `issue_id`.\n     - Logic: Get Issue -> Generate Prompt -> Create Session -> Comment on Issue.\n  2. Update `jules-go` CLI:\n     - Add `delegate` command support.\n\nC. jules env (JULES-12)\n- Goal: Declarative Environment Management (IaC).\n- Tasks:\n  1. Create Windmill Script `f/jules/update_repo_env`.\n     - Logic: Browser Automation (Jules Web GUI) to paste setup script.\n  2. Update `jules-go` CLI:\n     - Add `env push` command.\n     - Reads `jules.yaml`, calls Windmill script.\n\nD. Auto-Close (JULES-7)\n- Goal: Close issues on PR merge.\n- Tasks:\n  1. Create Windmill Webhook `f/jules/github_webhook`.\n     - Trigger: `pull_request.closed` (merged).\n     - Logic: Parse \"Fixes #ID\", checkbox update, state transition.\n  2. Implement YouTrack Workflow `workflows/safe-merge.js` in `infrastruct/configs/youtrack.conf`.\n\nExecution Guidelines:\n- Test Driven Development.\n- Windmill First Architecture.\n- Update Documentation.",
    "url": "https://jules.google.com/session/4324009622645823838",
    "id": "4324009622645823838"
  },
  {
    "name": "sessions/10558238909647649340",
    "title": "Resolve Conflicts: PRs #20, #21",
    "createTime": "2026-01-19T17:14:44.355556324Z",
    "updateTime": "2026-01-20T04:38:51.486137Z",
    "state": "IN_PROGRESS",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "main"
      },
      "environmentVariablesEnabled": true
    },
    "prompt": "Resolve Conflicts and Merge: PR #20 and #21 (Cleanup)\n\nThese are cleanup PRs that have been waiting for merge.\n\n## PR #20: Structured Logging (if exists)\nCheck if branch exists, rebase onto main, resolve conflicts, merge.\n\n## PR #21: Dead Code Cleanup (rsrch)\nBranch: jule-rsrch-cleanup-14183345458803382220 (check if exists)\nIf branch is deleted, close the PR as stale.\nIf branch exists:\n1. Rebase onto main\n2. Resolve conflicts\n3. Push and merge\n\nThis is TOOLS-52 cleanup work.\n\nVerify rsrch builds after merge.",
    "url": "https://jules.google.com/session/10558238909647649340",
    "id": "10558238909647649340"
  },
  {
    "name": "sessions/17909672099881343750",
    "title": "Finalize PR #65 (Angrav History). Review branch 'bundle-11-angrav-enhanced-history-scraping-11481867437687337743', verify scraping logic, ensure tests pass, and mark ready for merge.",
    "createTime": "2026-01-19T06:37:40.611982401Z",
    "updateTime": "2026-01-19T07:51:13.807110Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "environmentVariablesEnabled": true
    },
    "prompt": "Finalize PR #65 (Angrav History). Review branch 'bundle-11-angrav-enhanced-history-scraping-11481867437687337743', verify scraping logic, ensure tests pass, and mark ready for merge.",
    "url": "https://jules.google.com/session/17909672099881343750",
    "id": "17909672099881343750"
  },
  {
    "name": "sessions/3984337718161282780",
    "title": "Implement PROJ-4: Tests. Create agents/proj/tests/ for ProjStore (JSON/FalkorDB). See https://napoveda.youtrack.cloud/issue/PROJ-4",
    "createTime": "2026-01-19T06:22:54.643745531Z",
    "updateTime": "2026-01-19T07:04:41.487502Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "environmentVariablesEnabled": true
    },
    "prompt": "Implement PROJ-4: Tests. Create agents/proj/tests/ for ProjStore (JSON/FalkorDB). See https://napoveda.youtrack.cloud/issue/PROJ-4",
    "url": "https://jules.google.com/session/3984337718161282780",
    "id": "3984337718161282780"
  },
  {
    "name": "sessions/16026028798065658100",
    "title": "TOOLS-60: Workflow Engine",
    "createTime": "2026-01-11T15:32:54.573404730Z",
    "updateTime": "2026-01-11T15:41:58.511610Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "main"
      }
    },
    "prompt": "TOOLS-60: Workflow Composition Engine for rsrch.\n\nLocation: agents/rsrch/\n\nCreate src/workflow/:\n- WorkflowEngine.ts - Execute multi-step workflows\n- WorkflowStep.ts - Step definitions\n- WorkflowParser.ts - Parse YAML workflow files\n\nSupport step types:\n- research: Run Gemini query\n- transform: Process output\n- store: Save to FalkorDB\n\nAdd CLI: rsrch workflow run <file.yaml>\nAdd tests.\n\nSee https://napoveda.youtrack.cloud/issue/TOOLS-60",
    "url": "https://jules.google.com/session/16026028798065658100",
    "id": "16026028798065658100"
  },
  {
    "name": "sessions/15034901540554752472",
    "title": "Fix PR #21 Conflicts",
    "createTime": "2026-01-11T00:56:55.177183926Z",
    "updateTime": "2026-01-11T14:34:41.271461Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "jules-cleanup-rsrch-src-14183345458803382220"
      }
    },
    "prompt": "Fix conflicts for PR #21 (Clean up dead code from rsrch/src). Rebase jules-cleanup-rsrch-src-14183345458803382220 on main, resolve conflicts, push.",
    "url": "https://jules.google.com/session/15034901540554752472",
    "id": "15034901540554752472"
  },
  {
    "name": "sessions/5440445186486573920",
    "title": "Fix PR #20 Conflicts",
    "createTime": "2026-01-11T00:56:54.402545648Z",
    "updateTime": "2026-01-11T14:36:17.696724Z",
    "state": "IN_PROGRESS",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "jules-log-TOOLS-51-130636254482069761"
      }
    },
    "prompt": "Fix conflicts for PR #20 (rsrch structured logging with pino). Rebase jules-log-TOOLS-51-130636254482069761 on main, resolve conflicts, push.",
    "url": "https://jules.google.com/session/5440445186486573920",
    "id": "5440445186486573920"
  },
  {
    "name": "sessions/6388280359394264421",
    "title": "Fix PR #51 Conflicts",
    "createTime": "2026-01-11T00:56:24.954042024Z",
    "updateTime": "2026-01-11T14:38:19.198459Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "main"
      }
    },
    "prompt": "Rebase and fix conflicts for PR #51 (Add CLI subcommands to jules-go). The branch has conflicts with main. Rebase on main, resolve conflicts, force push to fix the PR.",
    "url": "https://jules.google.com/session/6388280359394264421",
    "id": "6388280359394264421"
  },
  {
    "name": "sessions/15197890044463410384",
    "title": "Bundle 10 test",
    "createTime": "2026-01-11T00:43:31.927868681Z",
    "updateTime": "2026-01-11T15:00:11.268022Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "main"
      }
    },
    "prompt": "Bundle 10 test",
    "url": "https://jules.google.com/session/15197890044463410384",
    "id": "15197890044463410384"
  },
  {
    "name": "sessions/3280881156582590012",
    "title": "NEW: Create YouTrack sync for jules-go to update issue states.",
    "createTime": "2026-01-10T01:22:26.564762941Z",
    "updateTime": "2026-01-10T02:10:07.651419Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf"
    },
    "prompt": "NEW: Create YouTrack sync for jules-go to update issue states.\n\nCreate agents/jules-go/internal/youtrack/client.go:\n- YouTrackClient with baseURL and token\n- GetIssue(id string) (*Issue, error)\n- UpdateIssueState(id, state string) error\n- AddComment(id, text string) error\n- Link issue state changes to session completion\n\nCreate agents/jules-go/internal/youtrack/client_test.go with mocked tests.\n\nAdd youtrack configuration to config.yaml:\n- base_url, token, default_fixed_state",
    "url": "https://jules.google.com/session/3280881156582590012",
    "id": "3280881156582590012"
  },
  {
    "name": "sessions/5105910593560402895",
    "title": "TOOLS-81: Create Makefile for jules-go at agents/jules-go/Makefile. Targets: build, test (with coverage), lint (golangci-lint), fmt, clean, docker, run. Include help target.",
    "createTime": "2026-01-10T00:12:39.794438326Z",
    "updateTime": "2026-01-10T00:59:31.727993Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf"
    },
    "prompt": "TOOLS-81: Create Makefile for jules-go at agents/jules-go/Makefile. Targets: build, test (with coverage), lint (golangci-lint), fmt, clean, docker, run. Include help target.",
    "url": "https://jules.google.com/session/5105910593560402895",
    "id": "5105910593560402895"
  },
  {
    "name": "sessions/5809005873698851176",
    "title": "TOOLS-77: Add unit tests for jules-go API client at agents/jules-go/client_test.go. Test NewClient, CreateSession, GetSession, ListActivities. Use httptest for mocking. Table-driven tests following Go best practices.",
    "createTime": "2026-01-10T00:11:44.675006110Z",
    "updateTime": "2026-01-10T00:28:53.730227Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf"
    },
    "prompt": "TOOLS-77: Add unit tests for jules-go API client at agents/jules-go/client_test.go. Test NewClient, CreateSession, GetSession, ListActivities. Use httptest for mocking. Table-driven tests following Go best practices.",
    "url": "https://jules.google.com/session/5809005873698851176",
    "id": "5809005873698851176"
  },
  {
    "name": "sessions/12986847442003090445",
    "title": "TOOLS-48: Improve error handling and add retry logic in agents/rsrch/src/",
    "createTime": "2026-01-09T23:28:56.443184076Z",
    "updateTime": "2026-01-10T01:00:30.917816Z",
    "state": "AWAITING_USER_FEEDBACK",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf"
    },
    "prompt": "TOOLS-48: Improve error handling and add retry logic in agents/rsrch/src/\n\nAdd proper try-catch blocks and retry logic (exponential backoff) for:\n1. FalkorDB operations\n2. External API calls (Gemini, Perplexity)\n3. Browser operations\n\nUse a consistent retry utility. Ensure 'npm run build' passes. Create a PR.",
    "url": "https://jules.google.com/session/12986847442003090445",
    "id": "12986847442003090445"
  },
  {
    "name": "sessions/3027547544390723358",
    "title": "Nomad & Self-Hosted YouTrack OCI ARM Ansible Deployment",
    "createTime": "2025-12-14T03:28:16.740419500Z",
    "updateTime": "2025-12-17T17:19:15.448521Z",
    "state": "IN_PROGRESS",
    "sourceContext": {
      "source": "sources/github/simik394/osobni_wf",
      "githubRepoContext": {
        "startingBranch": "main"
      }
    },
    "prompt": "there is a new empty folder named integrations and inside is the folder for first such with a vision containing document. YOuu need to follow the direction for solution which was proposed there in the single present doc. firstly i need you to focus on the youtrack. I want a selfhosted version. my stack will be nomad, ansible, obsidian-remote, all runing on OCI arm always free instanci. make sure all the security nescessities are solved. as for the nomad. i have currently zero running services because i still havent implemented anything nomad. so i would also need a setup for that, including the documentation for users and dev docs. nomad setup naturally meant also setup of consul and vault. currently there should be just the computers already present in the inventory. \nbtw. can you do even some testing since you do not have my ssh key? \nnevermind the setup documentation which i will have to do manually since i assume you can not easily automate the secrets setup, can you?\ngive me ideally a single command i run with ansible that would fit my needs\n\n",
    "url": "https://jules.google.com/session/3027547544390723358",
    "id": "3027547544390723358",
    "outputs": [
      {
        "changeSet": {
          "source": "sources/github/simik394/osobni_wf",
          "gitPatch": {
            "unidiffPatch": "diff --git a/infrastruct/nomad_stack/README.md b/infrastruct/nomad_stack/README.md\nnew file mode 100644\nindex 0000000..fa38a87\n--- /dev/null\n+++ b/infrastruct/nomad_stack/README.md\n@@ -0,0 +1,120 @@\n+# Infrastructure Deployment\n+\n+This project automates the deployment of a self-hosted stack comprising **Nomad, Consul, Vault, YouTrack, Obsidian-Remote, and n8n** on an Ubuntu server (specifically targeting OCI ARM instances).\n+\n+## Components\n+\n+- **Ansible**: Orchestrates the setup.\n+- **HashiCorp Stack**: Nomad (Orchestrator), Consul (Service Discovery/Mesh), Vault (Secrets).\n+- **Services**:\n+    - **YouTrack**: Project management and issue tracking.\n+    - **Obsidian-Remote**: Running Obsidian via web interface.\n+    - **n8n**: Workflow automation.\n+    - **Traefik**: Reverse proxy for routing traffic.\n+\n+## Prerequisites\n+\n+- **Ansible** installed on your local machine.\n+- **SSH Access** to the target server (`halvarm` in the inventory). Ensure your `~/.ssh/config` has an alias for `halvarm` or update `infrastruct/nomad_stack/inventory.yml` with the IP/hostname.\n+- **Git** to clone this repository.\n+## Quick Start (Zero-Touch Deployment)\n+\n+To deploy the entire stack, run the following command from the repository root:\n+\n+```bash\n+ansible-playbook -i infrastruct/nomad_stack/inventory.yml infrastruct/nomad_stack/playbook.yml\n+```\n+\n+This single command will:\n+1.  **Detect Public IP**: Automatically find your server's OCI public IP for configuring routing.\n+2.  **Install & Setup Tailscale**: Automatically installs Tailscale to create a secure mesh network between your Laptop and the Cloud Server. **You will be prompted to authenticate** (run `sudo tailscale up` manually) on the first run.\n+3.  **Harden Security**: Configure UFW firewall, install Fail2Ban, and setup QEMU for ARM emulation.\n+4.  **Install & Configure HashiCorp Stack**: Nomad, Consul, and Vault.\n+    *   **Cloud Server**: Acts as the primary cluster leader (Voting Server).\n+    *   **Laptop**: Joins as a \"Non-Voting Server\" (Client + Control Plane), allowing you to submit jobs and view state without breaking cluster quorum if you go offline.\n+5.  **Auto-Init Vault**: \n+    *   Automatically initializes Vault.\n+    *   Saves the **Unseal Keys and Root Token** to a file named `vault_keys.json` in your local directory (on your laptop, not the server).\n+    *   **Automatically Unseals** Vault so it's ready to use immediately.\n+5.  **Deploy Jobs**: Launches Traefik, YouTrack, Obsidian-Remote, and n8n.\n+\n+**After the command finishes, your stack is live!**\n+\n+### Secrets Management\n+The Ansible playbook generates a file named `vault_keys.json` in your current directory. \n+**IMPORTANT**: Move this file to a secure location (Password Manager, encrypted volume) immediately. It contains the keys to your Kingdom.\n+\n+### Access Services\n+The services are exposed via Traefik. By default, they are configured to use `nip.io` with the server's public IP.\n+\n+- **Traefik Dashboard**: `http://<SERVER_IP>:8080` (tunneling recommended)\n+- **YouTrack**: `http://youtrack.<SERVER_IP>.nip.io`\n+- **Obsidian**: `http://obsidian.<SERVER_IP>.nip.io`\n+- **n8n**: `http://n8n.<SERVER_IP>.nip.io`\n+\n+## Production Configuration: Custom Domain & HTTPS\n+\n+For a secure production environment, you should move away from `nip.io` and HTTP. Follow these steps to configure a custom domain and enable automatic HTTPS (SSL) via Let's Encrypt.\n+\n+### 1. DNS Configuration\n+Point your custom domain (e.g., `youtrack.example.com`, `obsidian.example.com`) to your server's **Public IP** using an **A Record** in your DNS provider's dashboard.\n+\n+### 2. Update Traefik Job (`traefik.nomad.hcl`)\n+Edit the Traefik job file (on the server at `/opt/nomad/jobs/traefik.nomad.hcl` or in your Ansible templates) to enable the ACME resolver.\n+\n+Uncomment the relevant lines in the `args` section:\n+\n+```hcl\n+args = [\n+  ...\n+  # Redirect HTTP to HTTPS\n+  \"--entrypoints.web.http.redirections.entryPoint.to=websecure\",\n+  \"--entrypoints.web.http.redirections.entryPoint.scheme=https\",\n+\n+  # Enable Let's Encrypt\n+  \"--certificatesresolvers.myresolver.acme.email=your-email@example.com\", # <--- Set your email\n+  \"--certificatesresolvers.myresolver.acme.storage=/letsencrypt/acme.json\",\n+  \"--certificatesresolvers.myresolver.acme.httpchallenge=true\",\n+  \"--certificatesresolvers.myresolver.acme.httpchallenge.entrypoint=web\",\n+]\n+```\n+\n+Then redeploy Traefik:\n+```bash\n+nomad job run /opt/nomad/jobs/traefik.nomad.hcl\n+```\n+\n+### 4. Update Service Jobs\n+Update your service jobs (YouTrack, Obsidian, etc.) to use your custom domain and request the certificate.\n+\n+Example `youtrack.nomad.hcl`:\n+\n+```hcl\n+service {\n+  tags = [\n+    \"traefik.enable=true\",\n+    # Use your real domain\n+    \"traefik.http.routers.youtrack.rule=Host(`youtrack.example.com`)\", \n+    \"traefik.http.routers.youtrack.entrypoints=websecure\",\n+    # Enable TLS using the resolver defined in Traefik\n+    \"traefik.http.routers.youtrack.tls.certresolver=myresolver\" \n+  ]\n+}\n+```\n+\n+Redeploy the service:\n+```bash\n+nomad job run /opt/nomad/jobs/youtrack.nomad.hcl\n+```\n+\n+## Troubleshooting\n+\n+-   **Check Nomad Status**: `nomad status`\n+-   **Check Logs**: `nomad alloc logs <alloc_id>`\n+\n+## Security Notes\n+\n+-   **Firewall**: Only ports 22 (SSH), 80 (HTTP), and 443 (HTTPS) are open globally.\n+-   **Internal Ports**: Nomad/Consul/Vault RPC ports are bound to 0.0.0.0 for internal communication but blocked by UFW from external access (verify this on your specific cloud provider's security list/security groups as well).\n+-   **Secrets**: Ensure Vault is unsealed after every reboot, or configure Auto-Unseal.\n+\ndiff --git a/infrastruct/nomad_stack/group_vars/servers.yml b/infrastruct/nomad_stack/group_vars/servers.yml\nnew file mode 100644\nindex 0000000..ccce5b2\n--- /dev/null\n+++ b/infrastruct/nomad_stack/group_vars/servers.yml\n@@ -0,0 +1,4 @@\n+ansible_user: ubuntu\n+ansible_ssh_private_key_file: ~/.ssh/id_rsa\n+# Set this to your OCI Public IP for correct Traefik routing\n+# public_ip: 123.456.789.0\ndiff --git a/infrastruct/nomad_stack/inventory.yml b/infrastruct/nomad_stack/inventory.yml\nnew file mode 100644\nindex 0000000..63e1e09\n--- /dev/null\n+++ b/infrastruct/nomad_stack/inventory.yml\n@@ -0,0 +1,32 @@\n+# For running ansible from a local machine (e.g., ntb) to provision 'halvarm'\n+servers:\n+  hosts:\n+    halvarm:\n+      ansible_user: ubuntu\n+      # ansible_host is inherited from ~/.ssh/config alias 'halvarm'\n+      \n+      # Federation Config: Cloud Region\n+      nomad_region: cloud\n+      nomad_datacenter: oci-eu\n+      \n+      # Both are now full servers for their respective regions\n+      nomad_role: server \n+      \n+      # Peer to join via WAN (Tailscale)\n+      wan_peer: ntb\n+\n+# For running ansible from 'halvarm' itself or the laptop 'ntb'\n+local:\n+  hosts:\n+    localhost:\n+      ansible_connection: local\n+      \n+      # Federation Config: Local Region\n+      nomad_region: local\n+      nomad_datacenter: laptop\n+      \n+      # Both are now full servers for their respective regions\n+      nomad_role: server\n+      \n+      # Peer to join via WAN (Tailscale)\n+      wan_peer: halvarm\ndiff --git a/infrastruct/nomad_stack/playbook.yml b/infrastruct/nomad_stack/playbook.yml\nnew file mode 100644\nindex 0000000..6742f09\n--- /dev/null\n+++ b/infrastruct/nomad_stack/playbook.yml\n@@ -0,0 +1,12 @@\n+---\n+- name: Deploy Self-Hosted Stack\n+  hosts: all\n+  become: true\n+  vars:\n+    ansible_python_interpreter: /usr/bin/python3\n+  roles:\n+    - role: common\n+    - role: tailscale\n+    - role: docker\n+    - role: hashicorp\n+    - role: nomad_jobs\ndiff --git a/infrastruct/nomad_stack/roles/common/tasks/main.yml b/infrastruct/nomad_stack/roles/common/tasks/main.yml\nnew file mode 100644\nindex 0000000..e3731dd\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/common/tasks/main.yml\n@@ -0,0 +1,108 @@\n+---\n+- name: Install common packages\n+  ansible.builtin.apt:\n+    name:\n+      - curl\n+      - wget\n+      - vim\n+      - git\n+      - unzip\n+      - ufw\n+      - fail2ban\n+      - jq\n+    state: present\n+    update_cache: yes\n+\n+- name: Install ARM emulation packages (QEMU)\n+  ansible.builtin.apt:\n+    name:\n+      - qemu-user-static\n+      - binfmt-support\n+    state: present\n+  when: ansible_architecture == 'aarch64'\n+\n+- name: Detect Public IP (if not set)\n+  ansible.builtin.uri:\n+    url: https://api.ipify.org\n+    return_content: yes\n+  register: ipify_result\n+  when: public_ip is not defined\n+\n+- name: Set public_ip fact\n+  ansible.builtin.set_fact:\n+    public_ip: \"{{ ipify_result.content }}\"\n+  when: public_ip is not defined and ipify_result.status == 200\n+\n+- name: Display Public IP\n+  ansible.builtin.debug:\n+    msg: \"Detected Public IP: {{ public_ip }}\"\n+\n+# --- Swap Configuration ---\n+# Critical for 1GB RAM instances running Java apps (YouTrack)\n+\n+- name: Check if swap file exists\n+  ansible.builtin.stat:\n+    path: /swapfile\n+  register: swap_file\n+\n+- name: Create swap file (4GB)\n+  ansible.builtin.command: fallocate -l 4G /swapfile\n+  when: not swap_file.stat.exists\n+\n+- name: Set swap file permissions\n+  ansible.builtin.file:\n+    path: /swapfile\n+    owner: root\n+    group: root\n+    mode: '0600'\n+\n+- name: Format swap file\n+  ansible.builtin.command: mkswap /swapfile\n+  when: not swap_file.stat.exists\n+\n+- name: Enable swap\n+  ansible.builtin.command: swapon /swapfile\n+  when: not swap_file.stat.exists\n+\n+- name: Persist swap in fstab\n+  ansible.builtin.lineinfile:\n+    path: /etc/fstab\n+    line: '/swapfile none swap sw 0 0'\n+    state: present\n+\n+- name: Set swappiness to 10 (prefer RAM, but use swap when needed)\n+  ansible.posix.sysctl:\n+    name: vm.swappiness\n+    value: '10'\n+    state: present\n+\n+- name: Set vfs_cache_pressure to 50\n+  ansible.posix.sysctl:\n+    name: vm.vfs_cache_pressure\n+    value: '50'\n+    state: present\n+\n+# --- Firewall ---\n+\n+- name: Setup UFW\n+  community.general.ufw:\n+    state: enabled\n+    policy: deny\n+    direction: incoming\n+\n+- name: Allow SSH\n+  community.general.ufw:\n+    rule: allow\n+    name: OpenSSH\n+\n+- name: Allow HTTP\n+  community.general.ufw:\n+    rule: allow\n+    port: '80'\n+    proto: tcp\n+\n+- name: Allow HTTPS\n+  community.general.ufw:\n+    rule: allow\n+    port: '443'\n+    proto: tcp\ndiff --git a/infrastruct/nomad_stack/roles/docker/tasks/main.yml b/infrastruct/nomad_stack/roles/docker/tasks/main.yml\nnew file mode 100644\nindex 0000000..b1df6b7\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/docker/tasks/main.yml\n@@ -0,0 +1,87 @@\n+---\n+# tasks file for docker - Official Docker CE Installation\n+\n+- name: Update apt cache\n+  become: true\n+  ansible.builtin.apt:\n+    update_cache: yes\n+    cache_valid_time: 3600\n+\n+- name: Uninstall old conflicting Docker packages\n+  become: true\n+  ansible.builtin.apt:\n+    name:\n+      - docker.io\n+      - docker-doc\n+      - docker-compose\n+      - docker-compose-v2\n+      - podman-docker\n+      - containerd\n+      - runc\n+    state: absent\n+\n+- name: Install prerequisite packages\n+  become: true\n+  ansible.builtin.apt:\n+    name:\n+      - ca-certificates\n+      - curl\n+      - gnupg\n+    state: present\n+\n+- name: Create directory for Docker keyrings\n+  become: true\n+  ansible.builtin.file:\n+    path: /etc/apt/keyrings\n+    state: directory\n+    mode: '0755'\n+\n+- name: Add Docker's official GPG key\n+  become: true\n+  ansible.builtin.shell:\n+    cmd: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n+    creates: /etc/apt/keyrings/docker.gpg\n+\n+- name: Set up the Docker repository\n+  become: true\n+  ansible.builtin.shell:\n+    cmd: |\n+      echo \\\n+      \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n+      $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n+      sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n+    creates: /etc/apt/sources.list.d/docker.list\n+\n+- name: Update apt cache (again)\n+  become: true\n+  ansible.builtin.apt:\n+    update_cache: yes\n+\n+- name: Install Docker Engine and plugins\n+  become: true\n+  ansible.builtin.apt:\n+    name:\n+      - docker-ce\n+      - docker-ce-cli\n+      - containerd.io\n+      - docker-buildx-plugin\n+      - docker-compose-plugin\n+    state: present\n+\n+- name: Start and enable Docker service\n+  become: true\n+  ansible.builtin.service:\n+    name: docker\n+    state: started\n+    enabled: true\n+\n+- name: Add the ubuntu user to the docker group\n+  become: true\n+  ansible.builtin.user:\n+    name: ubuntu\n+    groups: docker\n+    append: true\n+\n+# Note: We don't need to manually install docker-compose binary anymore\n+# because docker-compose-plugin provides 'docker compose' command.\n+\ndiff --git a/infrastruct/nomad_stack/roles/hashicorp/handlers/main.yml b/infrastruct/nomad_stack/roles/hashicorp/handlers/main.yml\nnew file mode 100644\nindex 0000000..48ffe35\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/hashicorp/handlers/main.yml\n@@ -0,0 +1,15 @@\n+---\n+- name: Restart Consul\n+  systemd:\n+    name: consul\n+    state: restarted\n+\n+- name: Restart Vault\n+  systemd:\n+    name: vault\n+    state: restarted\n+\n+- name: Restart Nomad\n+  systemd:\n+    name: nomad\n+    state: restarted\ndiff --git a/infrastruct/nomad_stack/roles/hashicorp/tasks/main.yml b/infrastruct/nomad_stack/roles/hashicorp/tasks/main.yml\nnew file mode 100644\nindex 0000000..b8dcc98\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/hashicorp/tasks/main.yml\n@@ -0,0 +1,241 @@\n+---\n+- name: Add HashiCorp GPG key\n+  ansible.builtin.shell: |\n+    wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\n+  args:\n+    creates: /usr/share/keyrings/hashicorp-archive-keyring.gpg\n+\n+- name: Add HashiCorp repository\n+  ansible.builtin.shell: |\n+    echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | tee /etc/apt/sources.list.d/hashicorp.list\n+  args:\n+    creates: /etc/apt/sources.list.d/hashicorp.list\n+\n+- name: Install Consul, Vault, Nomad\n+  ansible.builtin.apt:\n+    name:\n+      - consul\n+      - vault\n+      - nomad\n+    state: present\n+    update_cache: yes\n+\n+- name: Create data directories\n+  ansible.builtin.file:\n+    path: \"{{ item }}\"\n+    state: directory\n+    owner: root\n+    group: root\n+    mode: '0755'\n+  loop:\n+    - /opt/consul\n+    - /opt/vault\n+    - /opt/nomad\n+\n+- name: Get Tailscale IP\n+  ansible.builtin.command: tailscale ip -4\n+  register: tailscale_ip\n+  changed_when: false\n+  failed_when: false\n+\n+- name: Set Bind IP Fact\n+  ansible.builtin.set_fact:\n+    bind_ip: \"{{ tailscale_ip.stdout | default(ansible_default_ipv4.address) }}\"\n+\n+- name: Configure Consul\n+  ansible.builtin.copy:\n+    dest: /etc/consul.d/consul.hcl\n+    content: |\n+      datacenter = \"{{ nomad_datacenter | default('dc1') }}\"\n+      data_dir = \"/opt/consul\"\n+      log_level = \"INFO\"\n+      node_name = \"{{ inventory_hostname }}\"\n+      server = true\n+      \n+      # Always bootstrap self as we are independent regions\n+      bootstrap_expect = 1\n+\n+      bind_addr = \"{{ bind_ip }}\"\n+      client_addr = \"0.0.0.0\"\n+      advertise_addr = \"{{ bind_ip }}\"\n+      \n+      # WAN Joining (Federation)\n+      retry_join_wan = [\"{{ wan_peer | default('') }}\"]\n+\n+      ui_config {\n+        enabled = true\n+      }\n+      connect {\n+        enabled = true\n+      }\n+      ports {\n+        grpc = 8502\n+      }\n+  notify: Restart Consul\n+\n+- name: Configure Vault\n+  ansible.builtin.copy:\n+    dest: /etc/vault.d/vault.hcl\n+    content: |\n+      ui = true\n+      storage \"consul\" {\n+        address = \"127.0.0.1:8500\"\n+        path    = \"vault/\"\n+      }\n+      listener \"tcp\" {\n+        address     = \"0.0.0.0:8200\"\n+        tls_disable = 1\n+      }\n+      api_addr = \"http://{{ bind_ip }}:8200\"\n+      cluster_addr = \"http://{{ bind_ip }}:8201\"\n+  notify: Restart Vault\n+\n+- name: Configure Nomad\n+  ansible.builtin.copy:\n+    dest: /etc/nomad.d/nomad.hcl\n+    content: |\n+      datacenter = \"{{ nomad_datacenter | default('dc1') }}\"\n+      region     = \"{{ nomad_region | default('global') }}\"\n+      data_dir = \"/opt/nomad\"\n+      bind_addr = \"0.0.0.0\"\n+      \n+      server {\n+        enabled = true\n+        bootstrap_expect = 1\n+        \n+        # Federation: Join other servers via WAN (Tailscale)\n+        server_join {\n+          retry_join = [\"{{ wan_peer | default('') }}\"] \n+        }\n+      }\n+      \n+      client {\n+        enabled = true\n+        network_interface = \"{{ ansible_default_ipv4.interface }}\" \n+        node_class = \"{{ node_class | default('unknown') }}\"\n+        meta {\n+          connect.sidecar_image = \"envoyproxy/envoy:v1.25.1\"\n+        }\n+        # Host volumes must be defined here if using host_volume stanza\n+        # But we are using docker binds in job files, so this is optional unless we switch.\n+      }\n+      \n+      plugin \"docker\" {\n+        config {\n+          allow_privileged = true\n+        }\n+      }\n+      \n+      consul {\n+        address = \"127.0.0.1:8500\"\n+      }\n+      \n+      vault {\n+        enabled = true\n+        address = \"http://127.0.0.1:8200\"\n+      }\n+      \n+      advertise {\n+        http = \"{{ bind_ip }}\"\n+        rpc  = \"{{ bind_ip }}\"\n+        serf = \"{{ bind_ip }}\"\n+      }\n+  notify: Restart Nomad\n+\n+- name: Enable and start services\n+  ansible.builtin.systemd:\n+    name: \"{{ item }}\"\n+    enabled: yes\n+    state: started\n+  loop:\n+    - consul\n+    - vault\n+    - nomad\n+\n+- name: Wait for Consul to start\n+  ansible.builtin.wait_for:\n+    port: 8500\n+    delay: 5\n+\n+- name: Wait for Vault to start\n+  ansible.builtin.wait_for:\n+    port: 8200\n+    delay: 5\n+\n+- name: Wait for Nomad to start\n+  ansible.builtin.wait_for:\n+    port: 4646\n+    delay: 5\n+\n+# --- Automated Vault Initialization & Unseal ---\n+\n+- name: Check Vault initialization status\n+  ansible.builtin.command: vault operator init -status -address=http://127.0.0.1:8200\n+  register: vault_init_status\n+  failed_when: vault_init_status.rc not in [0, 2]\n+  changed_when: false\n+  environment:\n+    VAULT_ADDR: \"http://127.0.0.1:8200\"\n+\n+- name: Initialize Vault (if not initialized)\n+  block:\n+    - name: Run Vault Init\n+      ansible.builtin.command: vault operator init -key-shares=1 -key-threshold=1 -format=json -address=http://127.0.0.1:8200\n+      register: vault_init_output\n+      environment:\n+        VAULT_ADDR: \"http://127.0.0.1:8200\"\n+    \n+    - name: Save Vault Keys to Local File\n+      ansible.builtin.copy:\n+        content: \"{{ vault_init_output.stdout }}\"\n+        dest: \"./vault_keys.json\"\n+        mode: '0600'\n+      delegate_to: localhost\n+      become: false\n+\n+    - name: Set facts for unsealing (newly initialized)\n+      ansible.builtin.set_fact:\n+        vault_unseal_key: \"{{ (vault_init_output.stdout | from_json).unseal_keys_b64[0] }}\"\n+        vault_root_token: \"{{ (vault_init_output.stdout | from_json).root_token }}\"\n+\n+  when: vault_init_status.rc == 2\n+\n+- name: Check Vault seal status\n+  ansible.builtin.command: vault status -format=json -address=http://127.0.0.1:8200\n+  register: vault_seal_status\n+  failed_when: false\n+  changed_when: false\n+  environment:\n+    VAULT_ADDR: \"http://127.0.0.1:8200\"\n+\n+- name: Read Vault keys from local file (if existing)\n+  block:\n+    - name: Read keys file\n+      ansible.builtin.command: cat ./vault_keys.json\n+      delegate_to: localhost\n+      become: false\n+      register: vault_keys_file\n+      changed_when: false\n+\n+    - name: Set facts for unsealing (from file)\n+      ansible.builtin.set_fact:\n+        vault_unseal_key: \"{{ (vault_keys_file.stdout | from_json).unseal_keys_b64[0] }}\"\n+\n+  when: \n+    - vault_init_status.rc == 0\n+    - (vault_seal_status.stdout | from_json).sealed == true\n+    - vault_unseal_key is not defined\n+\n+- name: Unseal Vault\n+  ansible.builtin.command: \"vault operator unseal -address=http://127.0.0.1:8200 {{ vault_unseal_key }}\"\n+  environment:\n+    VAULT_ADDR: \"http://127.0.0.1:8200\"\n+  when: \n+    - (vault_seal_status.stdout | from_json).sealed == true\n+    - vault_unseal_key is defined\n+  register: unseal_result\n+\n+- name: Display Root Token (Only if newly initialized)\n+  ansible.builtin.debug:\n+    msg: \"Vault Root Token: {{ vault_root_token }}\"\n+  when: vault_root_token is defined\ndiff --git a/infrastruct/nomad_stack/roles/nomad_jobs/tasks/main.yml b/infrastruct/nomad_stack/roles/nomad_jobs/tasks/main.yml\nnew file mode 100644\nindex 0000000..42d3c47\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/nomad_jobs/tasks/main.yml\n@@ -0,0 +1,96 @@\n+---\n+- name: Ensure jobs directory exists\n+  ansible.builtin.file:\n+    path: /opt/nomad/jobs\n+    state: directory\n+    mode: '0755'\n+\n+- name: Create Traefik ACME directory with secure permissions\n+  ansible.builtin.file:\n+    path: /opt/traefik/acme\n+    state: directory\n+    mode: '0600'\n+    owner: root\n+    group: root\n+\n+- name: Create YouTrack data directories with correct permissions (UID 13001)\n+  ansible.builtin.file:\n+    path: \"{{ item }}\"\n+    state: directory\n+    owner: 13001\n+    group: 13001\n+    mode: '0750'\n+  loop:\n+    - /opt/youtrack/data\n+    - /opt/youtrack/conf\n+    - /opt/youtrack/logs\n+    - /opt/youtrack/backups\n+\n+- name: Create n8n data directory with correct permissions (UID 1000)\n+  ansible.builtin.file:\n+    path: /opt/n8n/data\n+    state: directory\n+    owner: 1000\n+    group: 1000\n+    mode: '0750'\n+\n+- name: Create Windmill data directory with correct permissions (UID 999 for Postgres default)\n+  ansible.builtin.file:\n+    path: /opt/windmill/postgres\n+    state: directory\n+    owner: 999\n+    group: 999\n+    mode: '0750'\n+\n+- name: Create Obsidian data directories with correct permissions (UID 1000)\n+  ansible.builtin.file:\n+    path: \"{{ item }}\"\n+    state: directory\n+    owner: 1000\n+    group: 1000\n+    mode: '0750'\n+  loop:\n+    - /opt/obsidian/config\n+    - /opt/obsidian/vaults\n+\n+- name: Copy Nomad job files\n+  ansible.builtin.template:\n+    src: \"{{ item }}\"\n+    dest: \"/opt/nomad/jobs/{{ item | basename | regex_replace('\\\\.j2$', '') }}\"\n+    mode: '0644'\n+  loop:\n+    - traefik.nomad.hcl.j2\n+    - youtrack.nomad.hcl.j2\n+    - obsidian-remote.nomad.hcl.j2\n+    - n8n.nomad.hcl.j2\n+    - windmill.nomad.hcl.j2\n+\n+- name: Run Traefik\n+  ansible.builtin.command: nomad job run /opt/nomad/jobs/traefik.nomad.hcl\n+  environment:\n+    NOMAD_ADDR: http://127.0.0.1:4646\n+  changed_when: false\n+\n+- name: Run YouTrack\n+  ansible.builtin.command: nomad job run /opt/nomad/jobs/youtrack.nomad.hcl\n+  environment:\n+    NOMAD_ADDR: http://127.0.0.1:4646\n+  changed_when: false\n+\n+- name: Run Obsidian Remote\n+  ansible.builtin.command: nomad job run /opt/nomad/jobs/obsidian-remote.nomad.hcl\n+  environment:\n+    NOMAD_ADDR: http://127.0.0.1:4646\n+  changed_when: false\n+\n+- name: Run n8n\n+  ansible.builtin.command: nomad job run /opt/nomad/jobs/n8n.nomad.hcl\n+  environment:\n+    NOMAD_ADDR: http://127.0.0.1:4646\n+  changed_when: false\n+\n+- name: Run Windmill\n+  ansible.builtin.command: nomad job run /opt/nomad/jobs/windmill.nomad.hcl\n+  environment:\n+    NOMAD_ADDR: http://127.0.0.1:4646\n+  changed_when: false\ndiff --git a/infrastruct/nomad_stack/roles/nomad_jobs/templates/n8n.nomad.hcl.j2 b/infrastruct/nomad_stack/roles/nomad_jobs/templates/n8n.nomad.hcl.j2\nnew file mode 100644\nindex 0000000..d408e92\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/nomad_jobs/templates/n8n.nomad.hcl.j2\n@@ -0,0 +1,53 @@\n+job \"n8n\" {\n+  datacenters = [\"dc1\"]\n+  type        = \"service\"\n+\n+  group \"n8n\" {\n+    count = 1\n+\n+    network {\n+      port \"http\" {\n+        to = 5678\n+      }\n+    }\n+\n+    task \"n8n\" {\n+      driver = \"docker\"\n+\n+      constraint {\n+        attribute = \"${node.class}\"\n+        value     = \"cloud\"\n+      }\n+\n+      config {\n+        image = \"n8nio/n8n:latest\"\n+        ports = [\"http\"]\n+        volumes = [\n+          \"/opt/n8n/data:/home/node/.n8n\"\n+        ]\n+      }\n+\n+      env {\n+        N8N_HOST = \"n8n.{{ public_ip | default(ansible_default_ipv4.address) }}.nip.io\"\n+        N8N_PORT = \"5678\"\n+        N8N_PROTOCOL = \"http\"\n+        WEBHOOK_URL = \"http://n8n.{{ public_ip | default(ansible_default_ipv4.address) }}.nip.io/\"\n+      }\n+\n+      resources {\n+        cpu    = 500\n+        memory = 512\n+      }\n+\n+      service {\n+        name = \"n8n\"\n+        port = \"http\"\n+        tags = [\n+          \"traefik.enable=true\",\n+          \"traefik.http.routers.n8n.rule=Host(`n8n.{{ public_ip | default(ansible_default_ipv4.address) }}.nip.io`) || PathPrefix(`/n8n`)\",\n+          \"traefik.http.routers.n8n.entrypoints=web\"\n+        ]\n+      }\n+    }\n+  }\n+}\ndiff --git a/infrastruct/nomad_stack/roles/nomad_jobs/templates/obsidian-remote.nomad.hcl.j2 b/infrastruct/nomad_stack/roles/nomad_jobs/templates/obsidian-remote.nomad.hcl.j2\nnew file mode 100644\nindex 0000000..742d858\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/nomad_jobs/templates/obsidian-remote.nomad.hcl.j2\n@@ -0,0 +1,54 @@\n+job \"obsidian-remote\" {\n+  datacenters = [\"dc1\"]\n+  type        = \"service\"\n+\n+  group \"obsidian\" {\n+    count = 1\n+\n+    network {\n+      port \"http\" {\n+        to = 8080\n+      }\n+    }\n+\n+    task \"obsidian\" {\n+      driver = \"docker\"\n+\n+      constraint {\n+        attribute = \"${node.class}\"\n+        value     = \"cloud\"\n+      }\n+\n+      config {\n+        image = \"ghcr.io/sglmr/obsidian-remote:latest\"\n+        ports = [\"http\"]\n+        volumes = [\n+           \"/opt/obsidian/config:/config\",\n+           \"/opt/obsidian/vaults:/vaults\"\n+        ]\n+      }\n+\n+      resources {\n+        cpu    = 1000\n+        memory = 1024\n+      }\n+\n+      service {\n+        name = \"obsidian\"\n+        port = \"http\"\n+        tags = [\n+          \"traefik.enable=true\",\n+          \"traefik.http.routers.obsidian.rule=Host(`obsidian.{{ public_ip | default(ansible_default_ipv4.address) }}.nip.io`) || PathPrefix(`/obsidian`)\",\n+          \"traefik.http.routers.obsidian.entrypoints=web\"\n+        ]\n+        check {\n+          name     = \"alive\"\n+          type     = \"http\"\n+          path     = \"/\"\n+          interval = \"10s\"\n+          timeout  = \"2s\"\n+        }\n+      }\n+    }\n+  }\n+}\ndiff --git a/infrastruct/nomad_stack/roles/nomad_jobs/templates/traefik.nomad.hcl.j2 b/infrastruct/nomad_stack/roles/nomad_jobs/templates/traefik.nomad.hcl.j2\nnew file mode 100644\nindex 0000000..14a8e6f\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/nomad_jobs/templates/traefik.nomad.hcl.j2\n@@ -0,0 +1,74 @@\n+job \"traefik\" {\n+  datacenters = [\"dc1\"]\n+  type        = \"service\"\n+\n+  group \"traefik\" {\n+    count = 1\n+\n+    network {\n+      port \"http\" {\n+        static = 80\n+      }\n+      port \"https\" {\n+        static = 443\n+      }\n+      port \"admin\" {\n+        static = 8080\n+      }\n+    }\n+\n+    # Create a volume for storing ACME certificates\n+    # Note: You must create this directory on the host first: mkdir -p /opt/traefik/acme\n+    task \"traefik\" {\n+      driver = \"docker\"\n+\n+      constraint {\n+        attribute = \"${node.class}\"\n+        value     = \"cloud\"\n+      }\n+\n+      config {\n+        image        = \"traefik:v2.10\"\n+        network_mode = \"host\"\n+        \n+        # Mount the volume for certificates\n+        volumes = [\n+           \"/opt/traefik/acme:/letsencrypt\"\n+        ]\n+\n+        args = [\n+          \"--api.insecure=true\",\n+          \"--providers.consulcatalog=true\",\n+          \"--providers.consulcatalog.exposedByDefault=false\",\n+          \"--providers.consulcatalog.endpoint.address=127.0.0.1:8500\",\n+          \"--entrypoints.web.address=:80\",\n+          \"--entrypoints.websecure.address=:443\",\n+          \n+          # --- HTTPS / Let's Encrypt Configuration (Uncomment to Enable) ---\n+          \n+          # 1. HTTP to HTTPS Redirection\n+          # \"--entrypoints.web.http.redirections.entryPoint.to=websecure\",\n+          # \"--entrypoints.web.http.redirections.entryPoint.scheme=https\",\n+\n+          # 2. Certificate Resolver (LetsEncrypt)\n+          # Replace 'your-email@example.com' with your actual email\n+          # \"--certificatesresolvers.myresolver.acme.email=your-email@example.com\",\n+          # \"--certificatesresolvers.myresolver.acme.storage=/letsencrypt/acme.json\",\n+          # \"--certificatesresolvers.myresolver.acme.httpchallenge=true\",\n+          # \"--certificatesresolvers.myresolver.acme.httpchallenge.entrypoint=web\",\n+        ]\n+      }\n+\n+      service {\n+        name = \"traefik\"\n+        check {\n+          name     = \"alive\"\n+          type     = \"tcp\"\n+          port     = \"http\"\n+          interval = \"10s\"\n+          timeout  = \"2s\"\n+        }\n+      }\n+    }\n+  }\n+}\ndiff --git a/infrastruct/nomad_stack/roles/nomad_jobs/templates/windmill.nomad.hcl.j2 b/infrastruct/nomad_stack/roles/nomad_jobs/templates/windmill.nomad.hcl.j2\nnew file mode 100644\nindex 0000000..fb38f5d\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/nomad_jobs/templates/windmill.nomad.hcl.j2\n@@ -0,0 +1,114 @@\n+job \"windmill\" {\n+  datacenters = [\"dc1\"]\n+  region      = \"cloud\" # Enforce running on Cloud Region for DB persistence\n+  type        = \"service\"\n+\n+  group \"windmill\" {\n+    count = 1\n+\n+    network {\n+      port \"http\" {\n+        to = 8000\n+      }\n+      port \"db\" {\n+        to = 5432\n+      }\n+    }\n+\n+    # Volume for Postgres Data\n+    # Requires: mkdir -p /opt/windmill/postgres\n+    task \"postgres\" {\n+      driver = \"docker\"\n+      \n+      config {\n+        image = \"postgres:14\"\n+        ports = [\"db\"]\n+        volumes = [\n+          \"/opt/windmill/postgres:/var/lib/postgresql/data\"\n+        ]\n+      }\n+\n+      env {\n+        POSTGRES_USER = \"windmill\"\n+        POSTGRES_PASSWORD = \"changeme123\" # In prod, use Vault!\n+        POSTGRES_DB = \"windmill\"\n+      }\n+      \n+      resources {\n+        cpu    = 500\n+        memory = 512\n+      }\n+      \n+      service {\n+        name = \"windmill-db\"\n+        port = \"db\"\n+        check {\n+          name     = \"alive\"\n+          type     = \"tcp\"\n+          interval = \"10s\"\n+          timeout  = \"2s\"\n+        }\n+      }\n+    }\n+\n+    task \"windmill-server\" {\n+      driver = \"docker\"\n+\n+      config {\n+        image = \"ghcr.io/windmill-labs/windmill:latest\"\n+        ports = [\"http\"]\n+      }\n+\n+      env {\n+        DATABASE_URL = \"postgres://windmill:changeme123@${NOMAD_ADDR_db}/windmill?sslmode=disable\"\n+        BASE_URL = \"http://windmill.{{ public_ip | default(ansible_default_ipv4.address) }}.nip.io\"\n+      }\n+\n+      resources {\n+        cpu    = 500\n+        memory = 512\n+      }\n+\n+      service {\n+        name = \"windmill\"\n+        port = \"http\"\n+        tags = [\n+          \"traefik.enable=true\",\n+          \"traefik.http.routers.windmill.rule=Host(`windmill.{{ public_ip | default(ansible_default_ipv4.address) }}.nip.io`) || PathPrefix(`/windmill`)\",\n+          \"traefik.http.routers.windmill.entrypoints=web\"\n+        ]\n+        check {\n+          name     = \"alive\"\n+          type     = \"http\"\n+          path     = \"/\"\n+          interval = \"10s\"\n+          timeout  = \"2s\"\n+        }\n+      }\n+    }\n+    \n+    # Optional: Separate Worker Task if needed for scale, \n+    # but for \"minimal\" setup, the server image usually includes a worker or we run one alongside.\n+    # Windmill architecture splits Server and Worker.\n+    \n+    task \"windmill-worker\" {\n+      driver = \"docker\"\n+      \n+      config {\n+        image = \"ghcr.io/windmill-labs/windmill:latest\"\n+        command = \"windmill\"\n+        args = [\"worker\"]\n+      }\n+      \n+      env {\n+         DATABASE_URL = \"postgres://windmill:changeme123@${NOMAD_ADDR_db}/windmill?sslmode=disable\"\n+         BASE_URL = \"http://windmill.{{ public_ip | default(ansible_default_ipv4.address) }}.nip.io\"\n+      }\n+      \n+      resources {\n+        cpu = 200\n+        memory = 256\n+      }\n+    }\n+  }\n+}\ndiff --git a/infrastruct/nomad_stack/roles/nomad_jobs/templates/youtrack.nomad.hcl.j2 b/infrastruct/nomad_stack/roles/nomad_jobs/templates/youtrack.nomad.hcl.j2\nnew file mode 100644\nindex 0000000..d1c81fa\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/nomad_jobs/templates/youtrack.nomad.hcl.j2\n@@ -0,0 +1,63 @@\n+job \"youtrack\" {\n+  datacenters = [\"dc1\"]\n+  type        = \"service\"\n+\n+  group \"youtrack\" {\n+    count = 1\n+\n+    network {\n+      port \"http\" {\n+        to = 8080\n+      }\n+    }\n+\n+    # Volume for persistence. \n+    # Note: Host volume must be configured in Nomad client config or use docker volume.\n+    # For simplicity, we use docker volume mount via config or bind mount.\n+    # Nomad host_volume requires client config update which we didn't do in hashicorp role yet.\n+    # So we use absolute path bind mount.\n+    \n+    task \"youtrack\" {\n+      driver = \"docker\"\n+\n+      constraint {\n+        attribute = \"${node.class}\"\n+        value     = \"cloud\"\n+      }\n+\n+      config {\n+        image = \"jetbrains/youtrack:2024.1.28299\" # Use a specific version or latest\n+        ports = [\"http\"]\n+        volumes = [\n+          \"/opt/youtrack/data:/opt/youtrack/data\",\n+          \"/opt/youtrack/conf:/opt/youtrack/conf\",\n+          \"/opt/youtrack/logs:/opt/youtrack/logs\",\n+          \"/opt/youtrack/backups:/opt/youtrack/backups\"\n+        ]\n+      }\n+\n+      resources {\n+        cpu    = 2000 # YouTrack is heavy\n+        memory = 4096 # Needs decent RAM. OCI ARM has 24GB, so this is fine.\n+      }\n+\n+      service {\n+        name = \"youtrack\"\n+        port = \"http\"\n+        tags = [\n+          \"traefik.enable=true\",\n+          \"traefik.http.routers.youtrack.rule=Host(`youtrack.{{ public_ip | default(ansible_default_ipv4.address) }}.nip.io`) || PathPrefix(`/youtrack`)\",\n+          # Assuming nip.io for easy access if no domain. Or user can override.\n+          \"traefik.http.routers.youtrack.entrypoints=web\"\n+        ]\n+        check {\n+          name     = \"alive\"\n+          type     = \"http\"\n+          path     = \"/\"\n+          interval = \"10s\"\n+          timeout  = \"2s\"\n+        }\n+      }\n+    }\n+  }\n+}\ndiff --git a/infrastruct/nomad_stack/roles/tailscale/tasks/main.yml b/infrastruct/nomad_stack/roles/tailscale/tasks/main.yml\nnew file mode 100644\nindex 0000000..035a7e5\n--- /dev/null\n+++ b/infrastruct/nomad_stack/roles/tailscale/tasks/main.yml\n@@ -0,0 +1,36 @@\n+---\n+- name: Add Tailscale GPG key\n+  ansible.builtin.shell: |\n+    mkdir -p --mode=0755 /usr/share/keyrings\n+    curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.noarmor.gpg | tee /usr/share/keyrings/tailscale-archive-keyring.gpg >/dev/null\n+  args:\n+    creates: /usr/share/keyrings/tailscale-archive-keyring.gpg\n+\n+- name: Add Tailscale repository\n+  ansible.builtin.shell: |\n+    curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.tailscale-keyring.list | tee /etc/apt/sources.list.d/tailscale.list\n+  args:\n+    creates: /etc/apt/sources.list.d/tailscale.list\n+\n+- name: Install Tailscale\n+  ansible.builtin.apt:\n+    name: tailscale\n+    state: present\n+    update_cache: yes\n+\n+- name: Enable Tailscale service\n+  ansible.builtin.systemd:\n+    name: tailscaled\n+    enabled: yes\n+    state: started\n+\n+- name: Check if Tailscale is authenticated\n+  ansible.builtin.command: tailscale status\n+  register: tailscale_status\n+  failed_when: false\n+  changed_when: false\n+\n+- name: Prompt for Tailscale Authentication\n+  ansible.builtin.pause:\n+    prompt: \"Tailscale is not authenticated on {{ inventory_hostname }}. Please run 'sudo tailscale up' on the host manually now, authenticate it, and then press Enter to continue.\"\n+  when: tailscale_status.rc != 0\ndiff --git a/infrastruct/nomad_stack/verify_deployment.sh b/infrastruct/nomad_stack/verify_deployment.sh\nnew file mode 100644\nindex 0000000..6bc7e9d\n--- /dev/null\n+++ b/infrastruct/nomad_stack/verify_deployment.sh\n@@ -0,0 +1,26 @@\n+#!/bin/bash\n+\n+HOST=\"halvarm\"\n+echo \"Verifying deployment on $HOST...\"\n+\n+# 1. Check System Services\n+echo -e \"\\n--- Checking System Services ---\"\n+ssh $HOST \"systemctl is-active nomad consul vault docker\"\n+\n+# 2. Check Nomad Status\n+echo -e \"\\n--- Checking Nomad Status ---\"\n+ssh $HOST \"nomad status\"\n+\n+# 3. Check Consul Members\n+echo -e \"\\n--- Checking Consul Members ---\"\n+ssh $HOST \"consul members\"\n+\n+# 4. Check Vault Status\n+echo -e \"\\n--- Checking Vault Status ---\"\n+ssh $HOST \"vault status\"\n+\n+# 5. Check Docker Containers (via Nomad)\n+echo -e \"\\n--- Checking Docker Containers ---\"\n+ssh $HOST \"docker ps\"\n+\n+echo -e \"\\nVerification Complete.\"\ndiff --git a/infrastruct/terraform/README.md b/infrastruct/terraform/README.md\nnew file mode 100644\nindex 0000000..a17a6bd\n--- /dev/null\n+++ b/infrastruct/terraform/README.md\n@@ -0,0 +1,64 @@\n+# Terraform Infrastructure for OCI\n+\n+This folder contains Terraform configuration to provision an **Always Free** instance on Oracle Cloud Infrastructure (OCI) suitable for the Nomad stack.\n+\n+## Resources Created\n+\n+- **VCN & Subnet**: Public network configuration.\n+- **Security List**: Firewall rules allowing SSH (22), HTTP (80), HTTPS (443), and Tailscale (UDP 41641).\n+- **Compute Instance**: `VM.Standard.E2.1.Micro` (1 CPU, 1GB RAM) with **50GB Storage**.\n+\n+## Prerequisites\n+\n+1.  **OCI Account**: You need an active Oracle Cloud account.\n+2.  **Terraform**: [Install Terraform](https://developer.hashicorp.com/terraform/downloads).\n+3.  **OCI API Keys**:\n+    *   Go to your OCI Console -> User Settings -> API Keys -> Add API Key.\n+    *   Download the private key and save it (e.g., `~/.oci/oci_api_key.pem`).\n+    *   Note down the `Tenancy OCID`, `User OCID`, `Fingerprint`, and `Region`.\n+\n+## Setup Instructions\n+\n+1.  **Initialize Terraform**:\n+    ```bash\n+    cd infrastruct/terraform\n+    terraform init\n+    ```\n+\n+2.  **Configure Variables**:\n+    *   Copy the example file:\n+        ```bash\n+        cp terraform.tfvars.example terraform.tfvars\n+        ```\n+    *   Edit `terraform.tfvars` and fill in your OCI details.\n+    *   **Availability Domain**: You can find this by running `oci iam availability-domain list` or looking in the OCI Console when creating a VM.\n+\n+    **Using Existing Infrastructure?**\n+    If you already have a VCN and Subnet, you can skip creating them. You have two options:\n+    \n+    *Option A: By Name (Easier)*\n+    ```hcl\n+    create_network       = false\n+    existing_vcn_name    = \"nomad-vcn\"\n+    existing_subnet_name = \"nomad-subnet\"\n+    ```\n+\n+    *Option B: By OCID (Precise)*\n+    ```hcl\n+    create_network       = false\n+    existing_subnet_id   = \"ocid1.subnet.oc1..aaaa...\"\n+    ```\n+\n+3.  **Deploy**:\n+    ```bash\n+    terraform apply\n+    ```\n+    *   Type `yes` to confirm.\n+\n+4.  **Post-Deploy**:\n+    *   Terraform will **automatically update** your Ansible inventory (`infrastruct/nomad_stack/inventory.yml`) and variables with the new server's IP.\n+    *   You can immediately run the Ansible playbook step.\n+\n+## Notes on \"Always Free\" Limits\n+*   The `VM.Standard.E2.1.Micro` is the x86 Always Free tier.\n+*   If you have access to ARM resources (`VM.Standard.A1.Flex`), you can change the `shape` in `main.tf` to get 4 CPUs and 24GB RAM, but availability is often limited. This config targets the x86 tier as requested (\"single core . single gb\").\ndiff --git a/infrastruct/terraform/group_vars.tpl b/infrastruct/terraform/group_vars.tpl\nnew file mode 100644\nindex 0000000..3b157d9\n--- /dev/null\n+++ b/infrastruct/terraform/group_vars.tpl\n@@ -0,0 +1,4 @@\n+ansible_user: ubuntu\n+ansible_ssh_private_key_file: ~/.ssh/id_rsa\n+# Automatically set by Terraform\n+public_ip: ${public_ip}\ndiff --git a/infrastruct/terraform/inventory.tpl b/infrastruct/terraform/inventory.tpl\nnew file mode 100644\nindex 0000000..10d1c6d\n--- /dev/null\n+++ b/infrastruct/terraform/inventory.tpl\n@@ -0,0 +1,17 @@\n+# For running ansible from a local machine (e.g., ntb) to provision 'halvarm'\n+servers:\n+  hosts:\n+    halvarm:\n+      ansible_host: ${public_ip}\n+      ansible_user: ${ssh_user}\n+      # ansible_ssh_private_key_file is in group_vars\n+      node_class: cloud\n+      nomad_role: server\n+\n+# For running ansible from 'halvarm' itself or the laptop 'ntb'\n+local:\n+  hosts:\n+    localhost:\n+      ansible_connection: local\n+      node_class: laptop\n+      nomad_role: non_voting_server\ndiff --git a/infrastruct/terraform/main.tf b/infrastruct/terraform/main.tf\nnew file mode 100644\nindex 0000000..bba8f56\n--- /dev/null\n+++ b/infrastruct/terraform/main.tf\n@@ -0,0 +1,208 @@\n+terraform {\n+  required_providers {\n+    oci = {\n+      source  = \"oracle/oci\"\n+      version = \">= 4.0.0\"\n+    }\n+  }\n+}\n+\n+provider \"oci\" {\n+  tenancy_ocid     = var.tenancy_ocid\n+  user_ocid        = var.user_ocid\n+  fingerprint      = var.fingerprint\n+  private_key_path = var.private_key_path\n+  region           = var.region\n+}\n+\n+# --- Network Logic (New vs Existing) ---\n+\n+# Lookup existing VCN by Name (if provided and create_network is false)\n+data \"oci_core_vcns\" \"existing_vcns\" {\n+  count          = (!var.create_network && var.existing_vcn_name != \"\") ? 1 : 0\n+  compartment_id = var.compartment_ocid\n+  display_name   = var.existing_vcn_name\n+}\n+\n+# Lookup existing Subnet by Name (if provided and create_network is false)\n+# We need VCN ID first, either from lookup or we assume user might know just subnet name? \n+# OCI core_subnets requires compartment_id and vcn_id.\n+# So if user provides just VCN Name + Subnet Name, we can find it.\n+data \"oci_core_subnets\" \"existing_subnets\" {\n+  count          = (!var.create_network && var.existing_vcn_name != \"\" && var.existing_subnet_name != \"\") ? 1 : 0\n+  compartment_id = var.compartment_ocid\n+  vcn_id         = data.oci_core_vcns.existing_vcns[0].virtual_networks[0].id\n+  display_name   = var.existing_subnet_name\n+}\n+\n+locals {\n+  # Determine final Subnet ID\n+  # Priority: \n+  # 1. New Network (create_network=true) -> oci_core_subnet.nomad_subnet[0].id\n+  # 2. Existing ID (create_network=false, existing_subnet_id set) -> var.existing_subnet_id\n+  # 3. Existing Name (create_network=false, existing_subnet_name set) -> data lookup\n+  final_subnet_id = var.create_network ? oci_core_subnet.nomad_subnet[0].id : (\n+    var.existing_subnet_id != \"\" ? var.existing_subnet_id : (\n+      length(data.oci_core_subnets.existing_subnets) > 0 ? data.oci_core_subnets.existing_subnets[0].subnets[0].id : \"\"\n+    )\n+  )\n+}\n+\n+\n+resource \"oci_core_vcn\" \"nomad_vcn\" {\n+  count          = var.create_network ? 1 : 0\n+  cidr_block     = \"10.0.0.0/16\"\n+  compartment_id = var.compartment_ocid\n+  display_name   = \"nomad-vcn\"\n+  dns_label      = \"nomad\"\n+}\n+\n+resource \"oci_core_internet_gateway\" \"nomad_ig\" {\n+  count          = var.create_network ? 1 : 0\n+  compartment_id = var.compartment_ocid\n+  display_name   = \"nomad-ig\"\n+  vcn_id         = oci_core_vcn.nomad_vcn[0].id\n+  enabled        = true\n+}\n+\n+resource \"oci_core_route_table\" \"nomad_rt\" {\n+  count          = var.create_network ? 1 : 0\n+  compartment_id = var.compartment_ocid\n+  vcn_id         = oci_core_vcn.nomad_vcn[0].id\n+  display_name   = \"nomad-rt\"\n+\n+  route_rules {\n+    destination       = \"0.0.0.0/0\"\n+    destination_type  = \"CIDR_BLOCK\"\n+    network_entity_id = oci_core_internet_gateway.nomad_ig[0].id\n+  }\n+}\n+\n+resource \"oci_core_security_list\" \"nomad_sl\" {\n+  count          = var.create_network ? 1 : 0\n+  compartment_id = var.compartment_ocid\n+  vcn_id         = oci_core_vcn.nomad_vcn[0].id\n+  display_name   = \"nomad-security-list\"\n+\n+  egress_security_rules {\n+    destination = \"0.0.0.0/0\"\n+    protocol    = \"all\"\n+  }\n+\n+  # SSH\n+  ingress_security_rules {\n+    protocol = \"6\" # TCP\n+    source   = \"0.0.0.0/0\"\n+    tcp_options {\n+      min = 22\n+      max = 22\n+    }\n+  }\n+\n+  # HTTP\n+  ingress_security_rules {\n+    protocol = \"6\" # TCP\n+    source   = \"0.0.0.0/0\"\n+    tcp_options {\n+      min = 80\n+      max = 80\n+    }\n+  }\n+\n+  # HTTPS\n+  ingress_security_rules {\n+    protocol = \"6\" # TCP\n+    source   = \"0.0.0.0/0\"\n+    tcp_options {\n+      min = 443\n+      max = 443\n+    }\n+  }\n+\n+  # Tailscale UDP (Direct connections)\n+  ingress_security_rules {\n+    protocol = \"17\" # UDP\n+    source   = \"0.0.0.0/0\"\n+    udp_options {\n+      min = 41641\n+      max = 41641\n+    }\n+  }\n+  \n+  # ICMP (Ping)\n+  ingress_security_rules {\n+    protocol = \"1\"\n+    source   = \"0.0.0.0/0\"\n+  }\n+}\n+\n+resource \"oci_core_subnet\" \"nomad_subnet\" {\n+  count             = var.create_network ? 1 : 0\n+  cidr_block        = \"10.0.1.0/24\"\n+  display_name      = \"nomad-subnet\"\n+  dns_label         = \"nomadsub\"\n+  security_list_ids = [oci_core_security_list.nomad_sl[0].id]\n+  compartment_id    = var.compartment_ocid\n+  vcn_id            = oci_core_vcn.nomad_vcn[0].id\n+  route_table_id    = oci_core_route_table.nomad_rt[0].id\n+}\n+\n+# --- Compute ---\n+\n+# Get latest Ubuntu 22.04 image\n+data \"oci_core_images\" \"ubuntu\" {\n+  compartment_id   = var.compartment_ocid\n+  operating_system = \"Canonical Ubuntu\"\n+  operating_system_version = \"22.04\"\n+  shape            = \"VM.Standard.E2.1.Micro\"\n+  sort_by          = \"TIMECREATED\"\n+  sort_order       = \"DESC\"\n+}\n+\n+resource \"oci_core_instance\" \"nomad_server\" {\n+  availability_domain = var.availability_domain\n+  compartment_id      = var.compartment_ocid\n+  display_name        = \"nomad-server\"\n+  shape               = \"VM.Standard.E2.1.Micro\"\n+\n+  create_vnic_details {\n+    subnet_id        = local.final_subnet_id\n+    display_name     = \"primaryvnic\"\n+    assign_public_ip = true\n+    hostname_label   = \"halvarm\"\n+  }\n+\n+  source_details {\n+    source_type             = \"image\"\n+    source_id               = data.oci_core_images.ubuntu.images[0].id\n+    boot_volume_size_in_gbs = 50 # 50GB Boot Volume\n+  }\n+\n+  metadata = {\n+    ssh_authorized_keys = var.ssh_public_key\n+  }\n+}\n+\n+# --- Post-Provisioning Automation ---\n+# Generate/Update the Ansible Inventory file with the new Public IP\n+\n+resource \"local_file\" \"ansible_inventory\" {\n+  content = templatefile(\"${path.module}/inventory.tpl\", {\n+    public_ip = oci_core_instance.nomad_server.public_ip\n+    ssh_user  = \"ubuntu\"\n+  })\n+  filename = \"${path.module}/../../infrastruct/nomad_stack/inventory.yml\"\n+}\n+\n+# Also update group_vars/servers.yml with the public IP\n+resource \"local_file\" \"ansible_group_vars\" {\n+  content = templatefile(\"${path.module}/group_vars.tpl\", {\n+    public_ip = oci_core_instance.nomad_server.public_ip\n+  })\n+  filename = \"${path.module}/../../infrastruct/nomad_stack/group_vars/servers.yml\"\n+}\n+\n+\n+output \"server_public_ip\" {\n+  value = oci_core_instance.nomad_server.public_ip\n+}\ndiff --git a/infrastruct/terraform/terraform.tfvars.example b/infrastruct/terraform/terraform.tfvars.example\nnew file mode 100644\nindex 0000000..c8ce2f1\n--- /dev/null\n+++ b/infrastruct/terraform/terraform.tfvars.example\n@@ -0,0 +1,8 @@\n+tenancy_ocid     = \"ocid1.tenancy.oc1..aaaa...\"\n+user_ocid        = \"ocid1.user.oc1..aaaa...\"\n+fingerprint      = \"12:34:56:...\"\n+private_key_path = \"~/.oci/oci_api_key.pem\"\n+region           = \"eu-frankfurt-1\"\n+compartment_ocid = \"ocid1.compartment.oc1..aaaa...\"\n+availability_domain = \"Xyz:EU-FRANKFURT-1-AD-1\"\n+ssh_public_key   = \"ssh-rsa AAAA...\"\ndiff --git a/infrastruct/terraform/variables.tf b/infrastruct/terraform/variables.tf\nnew file mode 100644\nindex 0000000..17fe32a\n--- /dev/null\n+++ b/infrastruct/terraform/variables.tf\n@@ -0,0 +1,63 @@\n+variable \"tenancy_ocid\" {\n+  description = \"OCI Tenancy OCID\"\n+  type        = string\n+}\n+\n+variable \"user_ocid\" {\n+  description = \"OCI User OCID\"\n+  type        = string\n+}\n+\n+variable \"fingerprint\" {\n+  description = \"OCI API Key Fingerprint\"\n+  type        = string\n+}\n+\n+variable \"private_key_path\" {\n+  description = \"Path to OCI API Private Key\"\n+  type        = string\n+}\n+\n+variable \"region\" {\n+  description = \"OCI Region (e.g., eu-frankfurt-1)\"\n+  type        = string\n+}\n+\n+variable \"compartment_ocid\" {\n+  description = \"Compartment OCID where resources will be created\"\n+  type        = string\n+}\n+\n+variable \"availability_domain\" {\n+  description = \"Availability Domain (e.g., Uocm:EU-FRANKFURT-1-AD-1)\"\n+  type        = string\n+}\n+\n+variable \"ssh_public_key\" {\n+  description = \"SSH Public Key content (e.g., 'ssh-rsa AAAA...')\"\n+  type        = string\n+}\n+\n+variable \"create_network\" {\n+  description = \"Set to false to use an existing VCN and Subnet\"\n+  type        = bool\n+  default     = true\n+}\n+\n+variable \"existing_subnet_id\" {\n+  description = \"OCID of the existing subnet (Optional if create_network=false and existing_vcn_name/subnet_name provided)\"\n+  type        = string\n+  default     = \"\"\n+}\n+\n+variable \"existing_vcn_name\" {\n+  description = \"Display Name of the existing VCN (Used for lookup if ID not provided)\"\n+  type        = string\n+  default     = \"\"\n+}\n+\n+variable \"existing_subnet_name\" {\n+  description = \"Display Name of the existing Subnet (Used for lookup if ID not provided)\"\n+  type        = string\n+  default     = \"\"\n+}\n",
            "baseCommitId": "f67b2cbb2b904d4dba9d63100df8ea71faea9c3c",
            "suggestedCommitMessage": "Add Windmill integration to Nomad stack\n\nAdded Windmill (Open Source Developer Platform) to the infrastructure.\n- Created `windmill.nomad.hcl.j2` job template including Postgres DB and Windmill Server.\n- Configured persistence for Windmill Postgres.\n- Integrated Windmill into `nomad_jobs` role and Ansible playbook.\n- Exposed Windmill via Traefik ingress."
          }
        }
      }
    ]
  }
]